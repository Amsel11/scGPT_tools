{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import pickle\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "#import scvi\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    dataset_name=\"ms\",\n",
    "    do_train=True,\n",
    "    load_model= r\"C:\\Users\\annel\\OneDrive\\Documenten\\Machine Learning\\scGPT_data\\Human\",\n",
    "    mask_ratio=0.0,\n",
    "    epochs=10,\n",
    "    n_bins=51,\n",
    "    MVC=False, # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.0, # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=0.0,\n",
    "    lr=1e-4,\n",
    "    batch_size=32,\n",
    "    layer_size=128,\n",
    "    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=4,  # number of heads in nn.MultiheadAttention\n",
    "    dropout=0.2,  # dropout probability\n",
    "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5,\n",
    "    fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    include_zero_gene = False,\n",
    "    freeze = False, #freeze\n",
    "    DSBN = False,  # Domain-spec batchnorm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = hyperparameter_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config[\"mask_ratio\"]\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "include_zero_gene = config[\"include_zero_gene\"]  # if True, include zero genes among hvgs in the training\n",
    "max_seq_len = 3001\n",
    "n_bins = config[\"n_bins\"]\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "output_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "\n",
    "# settings for training\n",
    "MLM = False  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = True  # celltype classification objective\n",
    "ADV = False  # Adversarial training for batch correction+\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "#MVC = config.MVC  # Masked value prediction for cell embedding\n",
    "#ECS = config.ecs_thres > 0  # Elastic cell similarity objective\n",
    "DAB = False  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer\n",
    "INPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\n",
    "input_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "adv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\n",
    "adv_D_delay_epochs = 0\n",
    "mvc_decoder_style = \"inner product\"\n",
    "ecs_threshold = config[\"ecs_thres\"]\n",
    "dab_weight = config[\"dab_weight\"]\n",
    "\n",
    "explicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\n",
    "do_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n",
    "\n",
    "per_seq_batch_sample = False\n",
    "\n",
    "# settings for optimizer\n",
    "lr = config[\"lr\"]  # TODO: test learning rate ratio between two tasks\n",
    "lr_ADV = 1e-3  # learning rate for discriminator, used when ADV is True\n",
    "batch_size = config[\"batch_size\"]\n",
    "eval_batch_size = config[\"batch_size\"]\n",
    "epochs = config[\"epochs\"] \n",
    "schedule_interval = 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% validate settings\n",
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "if ADV and DAB:\n",
    "    raise ValueError(\"ADV and DAB cannot be both True.\")\n",
    "DAB_separate_optim = True if DAB > 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save\\dev_ms-Jan23-16-47\n"
     ]
    }
   ],
   "source": [
    "import scgpt as scg\n",
    "import time \n",
    "dataset_name = config[\"dataset_name\"]\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data\n",
    "In this part we find \n",
    "- Data loading of dataset name\n",
    "- making the bach category columns\n",
    "- loading the model dir and vocab and args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this step we: \n",
    "1. Standardize their cell type labels\n",
    "2. Adding batch information to track data sources\n",
    "3. Organizing gene names as indices\n",
    "4. Setting up processing flags\n",
    "5. creating a backup\n",
    "Combining the datasets \n",
    "\"\"\"\n",
    "if dataset_name == \"ms\":\n",
    "    data_dir = Path(r\"C:\\Users\\annel\\OneDrive\\Documenten\\Machine Learning\\scGPT_data\\ms\")\n",
    "    adata = sc.read(data_dir / \"c_data.h5ad\") #this is the reference data \n",
    "    #adata_test = sc.read(data_dir / \"filtered_ms_adata.h5ad\") #this is the query data \n",
    "    adata.obs[\"celltype\"] = adata.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\") #adding a new column \"celltype\"\n",
    "    #adata_test.obs[\"celltype\"] = adata_test.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\") #adding a new column \"celltype\" for the dquery data\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\" #creates 2 identical columns in the metadata which tracks from which dataset each cell comes \n",
    "    #adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"     #for the query data this is stored as 1      \n",
    "    adata.var.set_index(adata.var[\"gene_name\"], inplace=True) #changes the index of the genes to the name of the gene \n",
    "    data_is_raw = False #Assuming it is already normalized / pre-processed / \n",
    "    filter_gene_by_counts = False #we don't filter any genes on their expression counts\n",
    "    #adata_test_raw = adata_test.copy() #make a safety backup of the test dataset (This is good practice!)\n",
    "    #adata = adata.concatenate(adata_test, batch_key=\"str_batch\") #this combines both the test and the training data\n",
    "                \n",
    "'''\n",
    "We concatenate so that we can create consistent category encodings across both datasets. \n",
    "'''\n",
    "\n",
    "# make the batch category column\n",
    "batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "celltype_id_labels = adata.obs[\"celltype\"].astype(\"category\").cat.codes.values\n",
    "celltypes = adata.obs[\"celltype\"].unique()\n",
    "num_types = len(np.unique(celltype_id_labels))\n",
    "id2type = dict(enumerate(adata.obs[\"celltype\"].astype(\"category\").cat.categories))\n",
    "adata.obs[\"celltype_id\"] = celltype_id_labels\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'PVALB-expressing interneuron', 1: 'SST-expressing interneuron', 2: 'SV2C-expressing interneuron', 3: 'VIP-expressing interneuron', 4: 'astrocyte', 5: 'cortical layer 2-3 excitatory neuron A', 6: 'cortical layer 2-3 excitatory neuron B', 7: 'cortical layer 4 excitatory neuron', 8: 'cortical layer 5-6 excitatory neuron', 9: 'endothelial cell', 10: 'microglial cell', 11: 'mixed excitatory neuron', 12: 'mixed glial cell?', 13: 'oligodendrocyte A', 14: 'oligodendrocyte C', 15: 'oligodendrocyte precursor cell', 16: 'phagocyte', 17: 'pyramidal neuron?'}\n",
      "[8 0 6 ... 0 7 1]\n",
      "['cortical layer 5-6 excitatory neuron', 'PVALB-expressing interneuron', 'cortical layer 2-3 excitatory neuron B', 'oligodendrocyte C', 'VIP-expressing interneuron', ..., 'astrocyte', 'microglial cell', 'endothelial cell', 'oligodendrocyte A', 'phagocyte']\n",
      "Length: 18\n",
      "Categories (18, object): ['PVALB-expressing interneuron', 'SST-expressing interneuron', 'SV2C-expressing interneuron', 'VIP-expressing interneuron', ..., 'oligodendrocyte C', 'oligodendrocyte precursor cell', 'phagocyte', 'pyramidal neuron?']\n",
      "18\n",
      "SRR9123032-AACACGTAGCGTAATA     8\n",
      "SRR9123032-AACCATGAGAAACGAG     0\n",
      "SRR9123032-AACCGCGAGGCCATAG     6\n",
      "SRR9123032-AACTCCCGTGTGACCC    14\n",
      "SRR9123032-AACTTTCCAGCTCGAC     3\n",
      "                               ..\n",
      "SRR9123041-TTTGCGCTCCACGACG     5\n",
      "SRR9123041-TTTGCGCTCCAGAGGA     7\n",
      "SRR9123041-TTTGCGCTCGTCTGAA     0\n",
      "SRR9123041-TTTGTCATCACTCCTG     7\n",
      "SRR9123041-TTTGTCATCGAATGCT     1\n",
      "Name: celltype_id, Length: 7844, dtype: int8\n"
     ]
    }
   ],
   "source": [
    "print(id2type)\n",
    "print(celltype_id_labels)\n",
    "print(celltypes)\n",
    "print(num_types)\n",
    "print(adata.obs[\"celltype_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "C:\\Users\\annel\\OneDrive\\Documenten\\Machine Learning\\scGPT_data\\Human C:\\Users\\annel\\OneDrive\\Documenten\\Machine Learning\\scGPT_data\\Human\\args.json C:\\Users\\annel\\OneDrive\\Documenten\\Machine Learning\\scGPT_data\\Human\\best_model.pt C:\\Users\\annel\\OneDrive\\Documenten\\Machine Learning\\scGPT_data\\Human\\vocab.json\n",
      "scGPT - INFO - match 2808/2808 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from C:\\Users\\annel\\OneDrive\\Documenten\\Machine Learning\\scGPT_data\\Human\\best_model.pt, the model args will override the config C:\\Users\\annel\\OneDrive\\Documenten\\Machine Learning\\scGPT_data\\Human\\args.json.\n"
     ]
    }
   ],
   "source": [
    "import shutil \n",
    "import json\n",
    "import numpy as np\n",
    "print (config[\"load_model\"] is not None)\n",
    "\n",
    "if config[\"load_model\"] is not None:\n",
    "    model_dir = Path(config[\"load_model\"])\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "    print(model_dir, model_config_file, model_file, vocab_file)\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# set up the preprocessor, use the args to config the workflow\n",
    "from scgpt.preprocess import Preprocessor\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "\n",
    "#adata_test = adata[adata.obs[\"str_batch\"] == \"1\"]\n",
    "adata = adata[adata.obs[\"str_batch\"] == \"0\"]\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n",
    "#preprocessor(adata_test, batch_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# get the training data\n",
    "\n",
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "celltypes_labels = adata.obs[\"celltype_id\"].tolist()  # make sure count from 0\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_celltype_labels,\n",
    "    valid_celltype_labels,\n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    ") = train_test_split(\n",
    "    all_counts, celltypes_labels, batch_ids, test_size=0.1, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785, 2808) (785,) (785,)\n",
      "[[ 0  0  0 ...  0  0  0]\n",
      " [42  0  0 ...  0  0  0]\n",
      " [23  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  6  0 ...  0  0  0]\n",
      " [40  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print (valid_data.shape, valid_celltype_labels.shape, valid_batch_labels.shape)\n",
    "print (valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inspection:\n",
      "Type of valid_data: <class 'numpy.ndarray'>\n",
      "Shape of valid_data: (785, 2808)\n",
      "Data type (dtype): int64\n",
      "\n",
      "First few values:\n",
      "[[ 0  0  0  0  0]\n",
      " [42  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Let's examine valid_data more closely\n",
    "print(\"Data inspection:\")\n",
    "print(f\"Type of valid_data: {type(valid_data)}\")\n",
    "print(f\"Shape of valid_data: {valid_data.shape}\")\n",
    "print(f\"Data type (dtype): {valid_data.dtype}\")\n",
    "\n",
    "# Make sure it's a proper numpy array with the right format\n",
    "if not isinstance(valid_data, np.ndarray):\n",
    "    valid_data = np.array(valid_data)\n",
    "    \n",
    "# Check the first few values again after conversion\n",
    "print(\"\\nFirst few values:\")\n",
    "print(valid_data[:2, :5])  # Show first 2 cells, first 5 genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")  # this is a pybind version of torchtext.vocab.Vocab\n",
    "\n",
    "if config[\"load_model\"] is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - valid set number of samples: 785, \n",
      "\t feature length: 1259\n"
     ]
    }
   ],
   "source": [
    "from scgpt.tokenizer import tokenize_and_pad_batch\n",
    "include_zero_gene = config[\"include_zero_gene\"]  # if True, include zero genes among hvgs in the training\n",
    "max_seq_len = 3001  # maximum sequence length, including <cls> and <eoc> tokens\n",
    "\n",
    "#from #validate settings\n",
    "\n",
    "pad_value = -2\n",
    "n_input_bins = n_bins\n",
    "\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "from torch.utils.data import Dataset, DataLoader    \n",
    "\n",
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    #masked_values_train = random_mask_value(\n",
    "    #    tokenized_train[\"values\"],\n",
    "    #    mask_ratio=mask_ratio,\n",
    "    #    mask_value=mask_value,\n",
    "    #    pad_value=pad_value,\n",
    "    #)\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    #print(\n",
    "    #    f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "    #    f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    #)\n",
    "\n",
    "    input_gene_ids_valid = (\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_valid = tokenized_valid[\"values\"]\n",
    "    target_values_valid = (\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    tensor_celltype_labels_train = torch.from_numpy(train_celltype_labels).long()\n",
    "    tensor_celltype_labels_valid = torch.from_numpy(valid_celltype_labels).long()\n",
    "\n",
    "    if sort_seq_batch:  # TODO: update to random pick seq source in each traning batch\n",
    "        #train_sort_ids = np.argsort(train_batch_labels)\n",
    "        #input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "        #input_values_train = input_values_train[train_sort_ids]\n",
    "        #target_values_train = target_values_train[train_sort_ids]\n",
    "        #tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "        #tensor_celltype_labels_train = tensor_celltype_labels_train[train_sort_ids]\n",
    "\n",
    "        valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "        input_values_valid = input_values_valid[valid_sort_ids]\n",
    "        target_values_valid = target_values_valid[valid_sort_ids]\n",
    "        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "        tensor_celltype_labels_valid = tensor_celltype_labels_valid[valid_sort_ids]\n",
    "\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "        \"batch_labels\": tensor_batch_labels_valid,\n",
    "        \"celltype_labels\": tensor_celltype_labels_valid,\n",
    "    }\n",
    "\n",
    "    return valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(os.cpu_count() or 1, batch_size // 2)\n",
    "\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - valid set number of samples: 785, \n",
      "\t feature length: 1259\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained scGPT model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): GeneEncoder(\n",
       "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
       "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_encoder): ContinuousValueEncoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.5, inplace=False)\n",
       "        (dropout2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ExprDecoder(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (cls_decoder): ClsDecoder(\n",
       "    (_decoder): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_layer): Linear(in_features=512, out_features=18, bias=True)\n",
       "  )\n",
       "  (sim): Similarity(\n",
       "    (cos): CosineSimilarity()\n",
       "  )\n",
       "  (creterion_cce): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TransformerModel(\n",
    "    len(vocab),\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=num_types if CLS else 1,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    ")\n",
    "\n",
    "if config[\"load_model\"] is not None:\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = torch.load(model_file, map_location=device)\n",
    "    pretrained_dict = {\n",
    "        k: v\n",
    "        for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and v.shape == model_dict[k].shape\n",
    "    }\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()  # Add this line for inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader, return_raw: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "            celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if INPUT_BATCH_LABELS or config[\"DSBN\"] else None,\n",
    "                    CLS=CLS,  # evaluation does not need CLS or CCE\n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=do_sample_in_train,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "                output_values = output_dict[\"cls_output\"]\n",
    "                loss = criterion_cls(output_values, celltype_labels)\n",
    "\n",
    "                if DAB:\n",
    "                    loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            accuracy = (output_values.argmax(1) == celltype_labels).sum().item()\n",
    "            total_error += (1 - accuracy / len(input_gene_ids)) * len(input_gene_ids)\n",
    "            total_dab += loss_dab.item() * len(input_gene_ids) if DAB else 0.0\n",
    "            total_num += len(input_gene_ids)\n",
    "            preds = output_values.argmax(1).cpu().numpy()\n",
    "            predictions.append(preds)\n",
    "\n",
    "\n",
    "    if return_raw:\n",
    "        return np.concatenate(predictions, axis=0)\n",
    "\n",
    "    return total_loss / total_num, total_error / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Import at the top of your file\n",
    "\n",
    "def evaluate1(model: nn.Module, loader: DataLoader, return_raw: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data with a progress indicator.\n",
    "    This version runs on CPU and shows real-time progress of the evaluation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    predictions = []\n",
    "    \n",
    "    # Calculate total number of batches for the progress bar\n",
    "    total_batches = len(loader)\n",
    "    \n",
    "    # Create progress bar with informative description\n",
    "    progress_bar = tqdm(\n",
    "        enumerate(loader),\n",
    "        total=total_batches,\n",
    "        desc=\"Evaluating model\",\n",
    "        # Add some useful metrics to display\n",
    "        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} batches '\n",
    "                  '[{elapsed}<{remaining}, {rate_fmt}]'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Use progress_bar instead of direct loader iteration\n",
    "        for batch_idx, batch_data in progress_bar:\n",
    "            # Get batch data\n",
    "            input_gene_ids = batch_data[\"gene_ids\"]\n",
    "            input_values = batch_data[\"values\"]\n",
    "            target_values = batch_data[\"target_values\"]\n",
    "            batch_labels = batch_data[\"batch_labels\"]\n",
    "            celltype_labels = batch_data[\"celltype_labels\"]\n",
    "            \n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            \n",
    "            # Handle config safely as before\n",
    "            use_batch_labels = (INPUT_BATCH_LABELS or \n",
    "                              (isinstance(config, dict) and config.get('DSBN', False)) or\n",
    "                              (hasattr(config, 'DSBN') and config[\"DSBN\"]))\n",
    "            \n",
    "            # Forward pass\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if use_batch_labels else None,\n",
    "                CLS=CLS,\n",
    "                CCE=False,\n",
    "                MVC=False,\n",
    "                ECS=False,\n",
    "                do_sample=do_sample_in_train,\n",
    "            )\n",
    "            \n",
    "            output_values = output_dict[\"cls_output\"]\n",
    "            loss = criterion_cls(output_values, celltype_labels)\n",
    "            \n",
    "            if DAB:\n",
    "                loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "            \n",
    "            # Calculate batch metrics\n",
    "            batch_size = len(input_gene_ids)\n",
    "            batch_loss = loss.item()\n",
    "            accuracy = (output_values.argmax(1) == celltype_labels).sum().item()\n",
    "            batch_error = 1 - accuracy / batch_size\n",
    "            \n",
    "            # Update totals\n",
    "            total_loss += batch_loss * batch_size\n",
    "            total_error += batch_error * batch_size\n",
    "            total_dab += loss_dab.item() * batch_size if DAB else 0.0\n",
    "            total_num += batch_size\n",
    "            \n",
    "            # Store predictions\n",
    "            preds = output_values.argmax(1).numpy()\n",
    "            predictions.append(preds)\n",
    "            \n",
    "            # Update progress bar with current metrics\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{batch_loss:.4f}',\n",
    "                'error': f'{batch_error:.4f}'\n",
    "            })\n",
    "    \n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "    \n",
    "    if return_raw:\n",
    "        return np.concatenate(predictions, axis=0)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    avg_loss = total_loss / total_num\n",
    "    avg_error = total_error / total_num\n",
    "    \n",
    "    print(f\"\\nEvaluation completed:\")\n",
    "    print(f\"Average loss: {avg_loss:.4f}\")\n",
    "    print(f\"Average error: {avg_error:.4f}\")\n",
    "    \n",
    "    return avg_loss, avg_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "valid_loader = prepare_dataloader(\n",
    "    valid_data_pt,\n",
    "    batch_size=eval_batch_size,\n",
    "    shuffle=False,\n",
    "    intra_domain_shuffle=False,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% inference\n",
    "def test(model: nn.Module, adata: DataLoader) -> float:\n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = adata.obs[\"celltype_id\"].tolist()  # make sure count from 0\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    tokenized_test = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=include_zero_gene,\n",
    "    )\n",
    "\n",
    "    input_values_test = random_mask_value(\n",
    "        tokenized_test[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "\n",
    "    test_data_pt = {\n",
    "        \"gene_ids\": tokenized_test[\"genes\"],\n",
    "        \"values\": input_values_test,\n",
    "        \"target_values\": tokenized_test[\"values\"],\n",
    "        \"batch_labels\": torch.from_numpy(batch_ids).long(),\n",
    "        \"celltype_labels\": torch.from_numpy(celltypes_labels).long(),\n",
    "    }\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=SeqDataset(test_data_pt),\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers = 0,\n",
    "        #num_workers= min(os.cpu_count() or 1, batch_size // 2),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    #\n",
    "    predictions = evaluate1(\n",
    "        model,\n",
    "        loader=test_loader,\n",
    "        return_raw=True,\n",
    "    )\n",
    "\n",
    "    # compute accuracy, precision, recall, f1\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    accuracy = accuracy_score(celltypes_labels, predictions)\n",
    "    precision = precision_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    recall = recall_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    macro_f1 = f1_score(celltypes_labels, predictions, average=\"macro\")\n",
    "\n",
    "    logger.info(\n",
    "        f\"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, \"\n",
    "        f\"Macro F1: {macro_f1:.3f}\"\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"test/accuracy\": accuracy,\n",
    "        \"test/precision\": precision,\n",
    "        \"test/recall\": recall,\n",
    "        \"test/macro_f1\": macro_f1,\n",
    "    }\n",
    "\n",
    "    return predictions, celltypes_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "criterion_mlm = criterion_neg_log_bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (encoder): GeneEncoder(\n",
      "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
      "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (value_encoder): ContinuousValueEncoder(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "    (activation): ReLU()\n",
      "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ExprDecoder(\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (cls_decoder): ClsDecoder(\n",
      "    (_decoder): ModuleList(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (out_layer): Linear(in_features=512, out_features=18, bias=True)\n",
      "  )\n",
      "  (sim): Similarity(\n",
      "    (cos): CosineSimilarity()\n",
      "  )\n",
      "  (creterion_cce): CrossEntropyLoss()\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m best_model \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m----> 3\u001b[0m predictions, label \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[44], line 55\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model, adata)\u001b[0m\n\u001b[0;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# compute accuracy, precision, recall, f1\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score\n",
      "Cell \u001b[1;32mIn[43], line 26\u001b[0m, in \u001b[0;36mevaluate1\u001b[1;34m(model, loader, return_raw)\u001b[0m\n\u001b[0;32m     23\u001b[0m src_key_padding_mask \u001b[38;5;241m=\u001b[39m input_gene_ids\u001b[38;5;241m.\u001b[39meq(vocab[pad_token])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Remove CUDA amp context since we're not using GPU\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_gene_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mINPUT_BATCH_LABELS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDSBN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCLS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCCE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMVC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mECS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample_in_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m output_values \u001b[38;5;241m=\u001b[39m output_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_output\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion_cls(output_values, celltype_labels)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\scgpt\\model\\model.py:345\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, src, values, src_key_padding_mask, batch_labels, CLS, CCE, MVC, ECS, do_sample)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    316\u001b[0m     src: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m     do_sample: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[0;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m        src (:obj:`Tensor`): token ids, shape [batch_size, seq_len]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03m        dict of output Tensors.\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 345\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batch_labels:\n\u001b[0;32m    349\u001b[0m         batch_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encoder(batch_labels)  \u001b[38;5;66;03m# (batch, embsize)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\scgpt\\model\\model.py:194\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[1;34m(self, src, values, src_key_padding_mask, batch_labels)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(total_embs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 194\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_embs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:387\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    384\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 387\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    390\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:678\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m why_not_sparsity_fast_path:\n\u001b[0;32m    677\u001b[0m         merged_mask, mask_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mmerge_masks(src_mask, src_key_padding_mask, src)\n\u001b[1;32m--> 678\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformer_encoder_layer_fwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m            \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_relu_or_gelu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmerged_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m x \u001b[38;5;241m=\u001b[39m src\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model = model\n",
    "print(model)\n",
    "predictions, label = test(best_model, adata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 0 6 ... 0 7 1]\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in adata.obs:\n",
      "Index(['Sample Characteristic[organism]',\n",
      "       'Sample Characteristic Ontology Term[organism]',\n",
      "       'Sample Characteristic[individual]',\n",
      "       'Sample Characteristic Ontology Term[individual]',\n",
      "       'Sample Characteristic[sex]',\n",
      "       'Sample Characteristic Ontology Term[sex]',\n",
      "       'Sample Characteristic[age]',\n",
      "       'Sample Characteristic Ontology Term[age]',\n",
      "       'Sample Characteristic[developmental stage]',\n",
      "       'Sample Characteristic Ontology Term[developmental stage]',\n",
      "       'Sample Characteristic[organism part]',\n",
      "       'Sample Characteristic Ontology Term[organism part]',\n",
      "       'Sample Characteristic[sampling site]',\n",
      "       'Sample Characteristic Ontology Term[sampling site]',\n",
      "       'Sample Characteristic[disease]',\n",
      "       'Sample Characteristic Ontology Term[disease]',\n",
      "       'Sample Characteristic[organism status]',\n",
      "       'Sample Characteristic Ontology Term[organism status]',\n",
      "       'Sample Characteristic[cause of death]',\n",
      "       'Sample Characteristic Ontology Term[cause of death]',\n",
      "       'Sample Characteristic[clinical history]',\n",
      "       'Sample Characteristic Ontology Term[clinical history]',\n",
      "       'Factor Value[disease]', 'Factor Value Ontology Term[disease]',\n",
      "       'Factor Value[sampling site]',\n",
      "       'Factor Value Ontology Term[sampling site]',\n",
      "       'Factor Value[inferred cell type - ontology labels]',\n",
      "       'Factor Value Ontology Term[inferred cell type - ontology labels]',\n",
      "       'Factor Value[inferred cell type - authors labels]',\n",
      "       'Factor Value Ontology Term[inferred cell type - authors labels]',\n",
      "       'str_batch', 'celltype', 'batch_id', 'celltype_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Show what columns are available in adata.obs\n",
    "print(\"Available columns in adata.obs:\")\n",
    "print(adata.obs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in adata.obs:\n",
      "Index(['Sample Characteristic[organism]',\n",
      "       'Sample Characteristic Ontology Term[organism]',\n",
      "       'Sample Characteristic[individual]',\n",
      "       'Sample Characteristic Ontology Term[individual]',\n",
      "       'Sample Characteristic[sex]',\n",
      "       'Sample Characteristic Ontology Term[sex]',\n",
      "       'Sample Characteristic[age]',\n",
      "       'Sample Characteristic Ontology Term[age]',\n",
      "       'Sample Characteristic[developmental stage]',\n",
      "       'Sample Characteristic Ontology Term[developmental stage]',\n",
      "       'Sample Characteristic[organism part]',\n",
      "       'Sample Characteristic Ontology Term[organism part]',\n",
      "       'Sample Characteristic[sampling site]',\n",
      "       'Sample Characteristic Ontology Term[sampling site]',\n",
      "       'Sample Characteristic[disease]',\n",
      "       'Sample Characteristic Ontology Term[disease]',\n",
      "       'Sample Characteristic[organism status]',\n",
      "       'Sample Characteristic Ontology Term[organism status]',\n",
      "       'Sample Characteristic[cause of death]',\n",
      "       'Sample Characteristic Ontology Term[cause of death]',\n",
      "       'Sample Characteristic[clinical history]',\n",
      "       'Sample Characteristic Ontology Term[clinical history]',\n",
      "       'Factor Value[disease]', 'Factor Value Ontology Term[disease]',\n",
      "       'Factor Value[sampling site]',\n",
      "       'Factor Value Ontology Term[sampling site]',\n",
      "       'Factor Value[inferred cell type - ontology labels]',\n",
      "       'Factor Value Ontology Term[inferred cell type - ontology labels]',\n",
      "       'Factor Value[inferred cell type - authors labels]',\n",
      "       'Factor Value Ontology Term[inferred cell type - authors labels]',\n",
      "       'str_batch', 'celltype', 'batch_id', 'celltype_id'],\n",
      "      dtype='object')\n",
      "\n",
      "Cell type information:\n",
      "Unique cell types from 'celltype' column:\n",
      "['cortical layer 5-6 excitatory neuron', 'PVALB-expressing interneuron', 'cortical layer 2-3 excitatory neuron B', 'oligodendrocyte C', 'VIP-expressing interneuron', ..., 'astrocyte', 'microglial cell', 'endothelial cell', 'oligodendrocyte A', 'phagocyte']\n",
      "Length: 18\n",
      "Categories (18, object): ['PVALB-expressing interneuron', 'SST-expressing interneuron', 'SV2C-expressing interneuron', 'VIP-expressing interneuron', ..., 'oligodendrocyte C', 'oligodendrocyte precursor cell', 'phagocyte', 'pyramidal neuron?']\n",
      "\n",
      "Mapping of cell type IDs to names:\n",
      "Cell type 0: PVALB-expressing interneuron (569 cells)\n",
      "Cell type 1: SST-expressing interneuron (172 cells)\n",
      "Cell type 2: SV2C-expressing interneuron (234 cells)\n",
      "Cell type 3: VIP-expressing interneuron (662 cells)\n",
      "Cell type 4: astrocyte (154 cells)\n",
      "Cell type 5: cortical layer 2-3 excitatory neuron A (2010 cells)\n",
      "Cell type 6: cortical layer 2-3 excitatory neuron B (1019 cells)\n",
      "Cell type 7: cortical layer 4 excitatory neuron (1284 cells)\n",
      "Cell type 8: cortical layer 5-6 excitatory neuron (997 cells)\n",
      "Cell type 9: endothelial cell (38 cells)\n",
      "Cell type 10: microglial cell (4 cells)\n",
      "Cell type 11: mixed excitatory neuron (114 cells)\n",
      "Cell type 12: mixed glial cell? (60 cells)\n",
      "Cell type 13: oligodendrocyte A (53 cells)\n",
      "Cell type 14: oligodendrocyte C (3 cells)\n",
      "Cell type 15: oligodendrocyte precursor cell (55 cells)\n",
      "Cell type 16: phagocyte (3 cells)\n",
      "Cell type 17: pyramidal neuron? (413 cells)\n"
     ]
    }
   ],
   "source": [
    "# First, let's see what columns we have in our AnnData object\n",
    "print(\"Columns in adata.obs:\")\n",
    "print(adata.obs.columns)\n",
    "\n",
    "# Let's look at how cell types are stored\n",
    "print(\"\\nCell type information:\")\n",
    "if 'celltype' in adata.obs.columns:\n",
    "    print(\"Unique cell types from 'celltype' column:\")\n",
    "    print(adata.obs['celltype'].unique())\n",
    "\n",
    "# Now let's create a mapping between IDs and names\n",
    "if 'celltype_id' in adata.obs.columns and 'celltype' in adata.obs.columns:\n",
    "    # Create a dictionary mapping IDs to names\n",
    "    id_to_name = {}\n",
    "    for idx, name in zip(adata.obs['celltype_id'], adata.obs['celltype']):\n",
    "        if idx not in id_to_name:\n",
    "            id_to_name[idx] = name\n",
    "    \n",
    "    print(\"\\nMapping of cell type IDs to names:\")\n",
    "    for id_num in sorted(id_to_name.keys()):\n",
    "        print(f\"Cell type {id_num}: {id_to_name[id_num]} ({len(adata.obs[adata.obs['celltype_id'] == id_num])} cells)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cell type categories:\n",
      "Index(['PVALB-expressing interneuron', 'SST-expressing interneuron',\n",
      "       'SV2C-expressing interneuron', 'VIP-expressing interneuron',\n",
      "       'astrocyte', 'cortical layer 2-3 excitatory neuron A',\n",
      "       'cortical layer 2-3 excitatory neuron B',\n",
      "       'cortical layer 4 excitatory neuron',\n",
      "       'cortical layer 5-6 excitatory neuron', 'endothelial cell',\n",
      "       'microglial cell', 'mixed excitatory neuron', 'mixed glial cell?',\n",
      "       'oligodendrocyte A', 'oligodendrocyte C',\n",
      "       'oligodendrocyte precursor cell', 'phagocyte', 'pyramidal neuron?'],\n",
      "      dtype='object')\n",
      "\n",
      "Mapping of IDs to cell types:\n",
      "ID 0: PVALB-expressing interneuron\n",
      "ID 1: SST-expressing interneuron\n",
      "ID 2: SV2C-expressing interneuron\n",
      "ID 3: VIP-expressing interneuron\n",
      "ID 4: astrocyte\n",
      "ID 5: cortical layer 2-3 excitatory neuron A\n",
      "ID 6: cortical layer 2-3 excitatory neuron B\n",
      "ID 7: cortical layer 4 excitatory neuron\n",
      "ID 8: cortical layer 5-6 excitatory neuron\n",
      "ID 9: endothelial cell\n",
      "ID 10: microglial cell\n",
      "ID 11: mixed excitatory neuron\n",
      "ID 12: mixed glial cell?\n",
      "ID 13: oligodendrocyte A\n",
      "ID 14: oligodendrocyte C\n",
      "ID 15: oligodendrocyte precursor cell\n",
      "ID 16: phagocyte\n",
      "ID 17: pyramidal neuron?\n"
     ]
    }
   ],
   "source": [
    "# If cell types are stored as categories, we can get the category names\n",
    "if 'celltype' in adata.obs.columns:\n",
    "    print(\"\\nCell type categories:\")\n",
    "    print(adata.obs['celltype'].cat.categories)\n",
    "\n",
    "# Match cell type IDs to names\n",
    "if 'celltype_id' in adata.obs.columns:\n",
    "    unique_ids = np.unique(adata.obs['celltype_id'])\n",
    "    print(\"\\nMapping of IDs to cell types:\")\n",
    "    for id_num in unique_ids:\n",
    "        # Get one example cell of this type to see its name\n",
    "        cell_type_name = adata.obs.loc[adata.obs['celltype_id'] == id_num, 'celltype'].iloc[0]\n",
    "        print(f\"ID {id_num}: {cell_type_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of predictions:\n",
      "Cell type 0: 569 cells\n",
      "Cell type 1: 172 cells\n",
      "Cell type 2: 234 cells\n",
      "Cell type 3: 662 cells\n",
      "Cell type 4: 154 cells\n",
      "Cell type 5: 2010 cells\n",
      "Cell type 6: 1019 cells\n",
      "Cell type 7: 1284 cells\n",
      "Cell type 8: 997 cells\n",
      "Cell type 9: 38 cells\n",
      "Cell type 10: 4 cells\n",
      "Cell type 11: 114 cells\n",
      "Cell type 12: 60 cells\n",
      "Cell type 13: 53 cells\n",
      "Cell type 14: 3 cells\n",
      "Cell type 15: 55 cells\n",
      "Cell type 16: 3 cells\n",
      "Cell type 17: 413 cells\n",
      "\n",
      "Cell type mapping:\n",
      "ID 0: PVALB-expressing interneuron\n",
      "ID 1: SST-expressing interneuron\n",
      "ID 2: SV2C-expressing interneuron\n",
      "ID 3: VIP-expressing interneuron\n",
      "ID 4: astrocyte\n",
      "ID 5: cortical layer 2-3 excitatory neuron A\n",
      "ID 6: cortical layer 2-3 excitatory neuron B\n",
      "ID 7: cortical layer 4 excitatory neuron\n",
      "ID 8: cortical layer 5-6 excitatory neuron\n",
      "ID 9: endothelial cell\n",
      "ID 10: microglial cell\n",
      "ID 11: mixed excitatory neuron\n",
      "ID 12: mixed glial cell?\n",
      "ID 13: oligodendrocyte A\n",
      "ID 14: oligodendrocyte C\n",
      "ID 15: oligodendrocyte precursor cell\n",
      "ID 16: phagocyte\n",
      "ID 17: pyramidal neuron?\n",
      "\n",
      "Performance Metrics:\n",
      "Accuracy: 1.000\n",
      "Precision: 1.000\n",
      "Recall: 1.000\n",
      "F1 Score: 1.000\n"
     ]
    }
   ],
   "source": [
    "# First, let's see the distribution of predicted cell types\n",
    "unique_labels, counts = np.unique(label, return_counts=True)\n",
    "print(\"Distribution of predictions:\")\n",
    "for lab, count in zip(unique_labels, counts):\n",
    "    print(f\"Cell type {lab}: {count} cells\")\n",
    "\n",
    "# If you want to see the actual cell type names instead of numbers\n",
    "# Assuming you have a mapping of ID to cell type name in your data\n",
    "celltype_names = adata.obs['celltype'].cat.categories\n",
    "print(\"\\nCell type mapping:\")\n",
    "for i, name in enumerate(celltype_names):\n",
    "    print(f\"ID {i}: {name}\")\n",
    "\n",
    "# To calculate metrics later, you'll need the true labels\n",
    "true_labels = np.array(adata.obs[\"celltype_id\"].tolist())\n",
    "\n",
    "# Then you can calculate metrics using:\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(true_labels, label)\n",
    "precision = precision_score(true_labels, label, average='macro')\n",
    "recall = recall_score(true_labels, label, average='macro')\n",
    "f1 = f1_score(true_labels, label, average='macro')\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene IDs shape: torch.Size([1, 252])\n",
      "First few tokens: tensor([60695,  4765, 17568, 31253, 21300, 34984, 11414,  5273, 32751,  3170])\n"
     ]
    }
   ],
   "source": [
    "sample_counts = all_counts[0:1]  # Take first sample\n",
    "tokenized_sample = tokenize_and_pad_batch(\n",
    "    sample_counts,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "print(\"Gene IDs shape:\", tokenized_sample[\"genes\"].shape)\n",
    "print(\"First few tokens:\", tokenized_sample[\"genes\"][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene IDs shape: torch.Size([1, 252])\n",
      "First few tokens: tensor([60695,  4765, 17568, 31253, 21300, 34984, 11414,  5273, 32751,  3170])\n"
     ]
    }
   ],
   "source": [
    "print(\"Gene IDs shape:\", tokenized_sample[\"genes\"].shape)\n",
    "print(\"First few tokens:\", tokenized_sample[\"genes\"][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_test_raw.obs[\"predictions\"] = [id2type[p] for p in predictions]\n",
    "\n",
    "# plot\n",
    "palette_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] \n",
    "palette_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] + plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] + plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "palette_ = {c: palette_[i] for i, c in enumerate(celltypes)}\n",
    "\n",
    "with plt.rc_context({\"figure.figsize\": (6, 4), \"figure.dpi\": (300)}):\n",
    "    sc.pl.umap(\n",
    "        adata_test_raw,\n",
    "        color=[\"celltype\", \"predictions\"],\n",
    "        palette=palette_,\n",
    "        show=False,\n",
    "    )\n",
    "    plt.savefig(save_dir / \"results.png\", dpi=300)\n",
    "\n",
    "save_dict = {\n",
    "    \"predictions\": predictions,\n",
    "    \"labels\": labels,\n",
    "    \"results\": results,\n",
    "    \"id_maps\": id2type\n",
    "}\n",
    "with open(save_dir / \"results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(save_dict, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
