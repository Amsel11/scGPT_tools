{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annel\\OneDrive\\Documenten\\Machine Learning\\scGPT_data\\scGPT_CP\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "import scanpy as sc\n",
    "import scib \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys \n",
    "\n",
    "import scgpt as scg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.context('default')\n",
    "warnings.simplefilter('ignore', ResourceWarning)\n",
    "\n",
    "model_dir = r\"C:\\Users\\annel\\OneDrive\\Documenten\\Machine Learning\\scGPT_data\\scGPT_CP\"\n",
    "print (model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Filter all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "repo_dir = Path.cwd().parent.absolute()\n",
    "sys.path.append(str(repo_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(repo_dir / \"GenePT-tools\"))\n",
    "from src.utils import setup_data_dir\n",
    "\n",
    "setup_data_dir()\n",
    "data_dir = repo_dir / \"data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "dataset = \"https://datasets.cellxgene.cziscience.com/10df7690-6d10-4029-a47e-0f071bb2df83.h5ad\"\n",
    "# dataset_id = \"10df7690-6d10-4029-a47e-0f071bb2df83\"\n",
    "\n",
    "file_path = data_dir / \"1m_cells.h5ad\"  # adjust this path as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_with_progress(url, file_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(file_path, 'wb') as file, tqdm(\n",
    "        desc=file_path.name,\n",
    "        total=total_size,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for data in response.iter_content(chunk_size=8192):\n",
    "            size = file.write(data)\n",
    "            pbar.update(size)\n",
    "\n",
    "# Usage\n",
    "if not file_path.exists():\n",
    "    download_with_progress(dataset, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of X group: ['data', 'indices', 'indptr']\n",
      "\n",
      "Contents of obs group: ['10X_run', '_index', '_scvi_batch', '_scvi_labels', 'ambient_removal', 'anatomical_position', 'assay', 'assay_ontology_term_id', 'broad_cell_class', 'cdna_plate', 'cdna_well', 'cell_type', 'cell_type_ontology_term_id', 'compartment', 'development_stage', 'development_stage_ontology_term_id', 'disease', 'disease_ontology_term_id', 'donor_assay', 'donor_id', 'donor_method', 'donor_tissue', 'donor_tissue_assay', 'ethnicity_original', 'free_annotation', 'is_primary_data', 'library_plate', 'manually_annotated', 'method', 'n_genes_by_counts', 'notes', 'observation_joinid', 'organism', 'organism_ontology_term_id', 'pct_counts_ercc', 'pct_counts_mt', 'published_2022', 'replicate', 'sample_id', 'sample_number', 'scvi_leiden_donorassay_full', 'self_reported_ethnicity', 'self_reported_ethnicity_ontology_term_id', 'sex', 'sex_ontology_term_id', 'suspension_type', 'tissue', 'tissue_in_publication', 'tissue_ontology_term_id', 'tissue_type', 'total_counts', 'total_counts_ercc', 'total_counts_mt']\n",
      "Contents of var group: ['ensembl_id', 'ensg', 'ercc', 'feature_biotype', 'feature_is_filtered', 'feature_length', 'feature_name', 'feature_reference', 'feature_type', 'genome', 'mean', 'mean_counts', 'mt', 'n_cells_by_counts', 'pct_dropout_by_counts', 'std', 'total_counts']\n",
      "\n",
      "Shape of X/data: (3537524007,)\n",
      "Shape of X/indices: (3537524007,)\n",
      "Shape of X/indptr: (1136219,)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    # Look at the structure of the X group\n",
    "    print(\"Contents of X group:\", list(f['X'].keys()))\n",
    "    \n",
    "    # Look at obs and var to get dimensions\n",
    "    print(\"\\nContents of obs group:\", list(f['obs'].keys()))\n",
    "    print(\"Contents of var group:\", list(f['var'].keys()))\n",
    "    \n",
    "    # If X contains a sparse matrix, it likely has 'data', 'indices', and 'indptr'\n",
    "    if 'data' in f['X']:\n",
    "        print(\"\\nShape of X/data:\", f['X']['data'].shape)\n",
    "        print(\"Shape of X/indices:\", f['X']['indices'].shape)\n",
    "        print(\"Shape of X/indptr:\", f['X']['indptr'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (100000, 61759)\n",
      "Matrix density: 0.045159779789180524\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def load_subset_sparse(h5py_file, start_row=0, n_rows=None):\n",
    "    \"\"\"\n",
    "    Load a subset of rows from the sparse matrix.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to h5ad file\n",
    "        start_row: Starting row index\n",
    "        n_rows: Number of rows to load\n",
    "    \n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix with the requested rows\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Get the indptr for the rows we want\n",
    "        if n_rows is None:\n",
    "            n_rows = len(f['X']['indptr']) - 1 - start_row\n",
    "\n",
    "        indptr = f['X']['indptr'][start_row:start_row + n_rows + 1]\n",
    "        # Find the indices in data array for our rows\n",
    "        start_idx = indptr[0]\n",
    "        end_idx = indptr[-1]\n",
    "        \n",
    "        # Load the relevant parts of the data and indices\n",
    "        data = f['X']['data'][start_idx:end_idx]\n",
    "        indices = f['X']['indices'][start_idx:end_idx]\n",
    "        \n",
    "        # Adjust indptr to start at 0\n",
    "        indptr = indptr - start_idx\n",
    "        \n",
    "        # Get the total number of columns from the var group\n",
    "        n_cols = len(f['var']['feature_name']['categories'])\n",
    "        \n",
    "        # Create the sparse matrix\n",
    "        return sparse.csr_matrix((data, indices, indptr), shape=(n_rows, n_cols))\n",
    "\n",
    "cell_gene_matrix = load_subset_sparse(file_path, start_row=0, n_rows=100000)\n",
    "print(\"Matrix shape:\", cell_gene_matrix.shape)\n",
    "print(\"Matrix density:\", cell_gene_matrix.nnz / (cell_gene_matrix.shape[0] * cell_gene_matrix.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.08 GiB for an array with shape (278902284,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m adata\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Load first 100,000 cells\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m adata_subset \u001b[38;5;241m=\u001b[39m \u001b[43mload_subset_scanpy_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 21\u001b[0m, in \u001b[0;36mload_subset_scanpy_chunked\u001b[1;34m(file_path, start_row, n_rows)\u001b[0m\n\u001b[0;32m     18\u001b[0m end_idx \u001b[38;5;241m=\u001b[39m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     20\u001b[0m data \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][start_idx:end_idx]\n\u001b[1;32m---> 21\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     22\u001b[0m indptr \u001b[38;5;241m=\u001b[39m indptr \u001b[38;5;241m-\u001b[39m start_idx\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Create sparse matrix\u001b[39;00m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\h5py\\_hl\\dataset.py:781\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, args, new_dtype)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 781\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fast_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "File \u001b[1;32mh5py\\\\_selector.pyx:368\u001b[0m, in \u001b[0;36mh5py._selector.Reader.read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_selector.pyx:342\u001b[0m, in \u001b[0;36mh5py._selector.Reader.make_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.08 GiB for an array with shape (278902284,) and data type int64"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "def load_subset_scanpy_chunked(file_path, start_row=0, n_rows=10000):\n",
    "    \"\"\"\n",
    "    Load a subset of rows from h5ad file using scanpy with chunked reading.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Get var names using ensembl_id\n",
    "        var_names = f['var']['ensembl_id'][:]\n",
    "        \n",
    "        # Get the sparse matrix data for the subset\n",
    "        indptr = f['X']['indptr'][start_row:start_row + n_rows + 1]\n",
    "        start_idx = indptr[0]\n",
    "        end_idx = indptr[-1]\n",
    "        \n",
    "        data = f['X']['data'][start_idx:end_idx]\n",
    "        indices = f['X']['indices'][start_idx:end_idx]\n",
    "        indptr = indptr - start_idx\n",
    "        \n",
    "        # Create sparse matrix\n",
    "        X = sparse.csr_matrix(\n",
    "            (data, indices, indptr),\n",
    "            shape=(n_rows, len(var_names))\n",
    "        )\n",
    "        \n",
    "        # Create basic AnnData object with var annotations\n",
    "        var_df = pd.DataFrame(index=var_names)\n",
    "        var_df['mean_counts'] = f['var']['mean_counts'][:]\n",
    "        var_df['n_cells'] = f['var']['n_cells_by_counts'][:]\n",
    "        \n",
    "        # Get cell metadata\n",
    "        obs_df = pd.DataFrame(index=range(start_row, start_row + n_rows))\n",
    "        \n",
    "        # Safely add obs annotations\n",
    "        obs_fields = ['total_counts', 'n_genes_by_counts', 'pct_counts_mt']\n",
    "        for field in obs_fields:\n",
    "            if field in f['obs'] and isinstance(f['obs'][field], h5py.Dataset):\n",
    "                obs_df[field] = f['obs'][field][start_row:start_row + n_rows]\n",
    "        \n",
    "        # Handle categorical data\n",
    "        if 'cell_type' in f['obs']:\n",
    "            if 'categories' in f['obs']['cell_type']:\n",
    "                categories = f['obs']['cell_type']['categories'][:]\n",
    "                indices = f['obs']['cell_type']['codes'][start_row:start_row + n_rows]\n",
    "                obs_df['cell_type'] = pd.Categorical.from_codes(indices, categories)\n",
    "        \n",
    "        adata = sc.AnnData(X=X, obs=obs_df, var=var_df)\n",
    "    \n",
    "    print(\"Matrix shape:\", adata.shape)\n",
    "    print(\"Matrix density:\", adata.X.nnz / (adata.shape[0] * adata.shape[1]))\n",
    "    print(\"\\nAvailable obs annotations:\", list(adata.obs.columns))\n",
    "    print(\"Available var annotations:\", list(adata.var.columns))\n",
    "    \n",
    "    return adata\n",
    "\n",
    "# Load first 100,000 cells\n",
    "adata_subset = load_subset_scanpy_chunked(file_path, start_row=0, n_rows=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"categories\": shape (61759,), type \"|O\">\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(file_path, 'r') as f:\n",
    "    print(f['var']['feature_name'][\"categories\"])\n",
    "    # print(f['X']['indices'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100000x61759 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 278902284 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_gene_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1136218\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(file_path, 'r') as f:\n",
    "    # Get the indptr for the rows we want\n",
    "    print(len(f['obs']['scvi_leiden_donorassay_full']['codes']))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with h5py.File(file_path, 'r') as f:\n",
    "    # Get the indptr for the rows we want\n",
    "    gene_names = f['var']['feature_name']\n",
    "    ensembl_ids = f['var']['ensembl_id']\n",
    "    scvi_leiden_donorassay_full = f['obs']['scvi_leiden_donorassay_full']['codes']\n",
    "    major_ensembl_ids = pd.Series(\n",
    "        ensembl_id.decode('utf-8').split('.')[0]\n",
    "        for ensembl_id in ensembl_ids\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
