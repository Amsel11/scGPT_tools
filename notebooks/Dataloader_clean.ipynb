{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\scgpt\\model\\model.py:21: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\scgpt\\model\\multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dataclasses import make_dataclass\n",
    "from typing import Any, Type, Dict, List, Tuple, Union\n",
    "import pprint\n",
    "\n",
    "import json\n",
    "import scgpt as scg\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "#from scgpt.utils import set_seed, category_str2int, eval_scib_metricsimport \n",
    "logger = scg.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    dataset_name=\"ms\",\n",
    "    do_train=True,\n",
    "    load_model=\"../save/scGPT_human\",\n",
    "    mask_ratio=0.0,\n",
    "    epochs=10,\n",
    "    n_bins=51,\n",
    "    MVC=False, # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.0, # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=0.0,\n",
    "    lr=1e-4,\n",
    "    batch_size=32,\n",
    "    layer_size=128,\n",
    "    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nheads=4,  # number of heads in nn.MultiheadAttention\n",
    "    dropout=0.2,  # dropout probability\n",
    "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5,\n",
    "    fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    include_zero_gene = False,\n",
    "    freeze = False, #freeze\n",
    "    DSBN = False,  # Domain-spec batchnorm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create config class to hold hyperparameters\n",
    "\n",
    "@dataclass #holds hyperparameters and data information for the model\n",
    "class ModelConfig: \n",
    "    @classmethod\n",
    "    def from_dict(hyperclass, config_dict: dict) -> 'ModelConfig':\n",
    "        # Create a new instance with all parameters from the dictionary\n",
    "        instance = hyperclass()\n",
    "        \n",
    "        # Set all attributes from the dictionary\n",
    "        for key, value in config_dict.items():\n",
    "            # Convert load_model to Path if it's a string\n",
    "            if key == 'load_model' and isinstance(value, str):\n",
    "                value = Path(value)\n",
    "            setattr(instance, key, value)\n",
    "        \n",
    "        return instance\n",
    "\n",
    "#Use: \n",
    "config = ModelConfig.from_dict(hyperparameter_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('seed', 0),\n",
      " ('dataset_name', 'ms'),\n",
      " ('do_train', True),\n",
      " ('load_model', WindowsPath('../save/scGPT_human')),\n",
      " ('mask_ratio', 0.0),\n",
      " ('epochs', 10),\n",
      " ('n_bins', 51),\n",
      " ('MVC', False),\n",
      " ('ecs_thres', 0.0),\n",
      " ('dab_weight', 0.0),\n",
      " ('lr', 0.0001),\n",
      " ('batch_size', 32),\n",
      " ('layer_size', 128),\n",
      " ('nlayers', 4),\n",
      " ('nheads', 4),\n",
      " ('dropout', 0.2),\n",
      " ('schedule_ratio', 0.9),\n",
      " ('save_eval_interval', 5),\n",
      " ('fast_transformer', True),\n",
      " ('pre_norm', False),\n",
      " ('amp', True),\n",
      " ('include_zero_gene', False),\n",
      " ('freeze', False),\n",
      " ('DSBN', False)]\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(width=200)\n",
    "pp.pprint(list(config.__dict__.items())) #list of all the hyperparameters and their values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_value = -2\n",
    "mask_value = -1\n",
    "n_input_bins = config.n_bins + 2\n",
    "\n",
    "# input/output representation\n",
    "''' \n",
    "Given in the paper, for cell annotation the input of the gene expression is binned \n",
    "line 705-706: The gene expression values were normalized, log-transformed, and binned prior to model fine-tuning\"\n",
    "'''\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "output_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "\n",
    "# Settings for input and preprocessing\n",
    "pad_token = \"<pad>\" #padding token ensuring all the same length \n",
    "special_tokens = [pad_token, \"cls\", \"<eoc>\"] #cls for cell type classification, eoc for end of cell\n",
    "mask_ratio = config.mask_ratio \n",
    "\n",
    "include_zero_gene = config.include_zero_gene #if true include zero genes among hvgs in the training\n",
    "max_seq_len = 3001 #max length for the gene expression. \n",
    "n_bins = config.n_bins #amount of bins for the binned gene expression values.\n",
    "\n",
    "#settings for evaluation\n",
    "eval_batch_size = config.batch_size #batch size for evaluation\n",
    "\n",
    "# settings for data \n",
    "num_batch_types = 1  # Default value if not using batch labels\n",
    "\n",
    "# settings for training\n",
    "MLM = False  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = True  # celltype classification objective\n",
    "ADV = False  # Adversarial training for batch correction\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = config.MVC  # Masked value prediction for cell embedding\n",
    "ECS = config.ecs_thres > 0  # Elastic cell similarity objective\n",
    "DAB = False  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer\n",
    "INPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\n",
    "input_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "adv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\n",
    "adv_D_delay_epochs = 0\n",
    "mvc_decoder_style = \"inner product\"\n",
    "ecs_threshold = config.ecs_thres\n",
    "dab_weight = config.dab_weight\n",
    "\n",
    "explicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\n",
    "do_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n",
    "\n",
    "per_seq_batch_sample = False\n",
    "\n",
    "# settings for the model\n",
    "fast_transformer = config.fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "embsize = config.layer_size  # embedding dimension\n",
    "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nheads = config.nheads  # number of heads in nn.MultiheadAttention\n",
    "dropout = config.dropout  # dropout probabilit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binned binned continuous cls\n",
      "51 -2 -1\n"
     ]
    }
   ],
   "source": [
    "# %% validate settings\n",
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "if ADV and DAB:\n",
    "    raise ValueError(\"ADV and DAB cannot be both True.\")\n",
    "DAB_separate_optim = True if DAB > 1 else False\n",
    "\n",
    "#print(input_style, output_style, input_emb_style, cell_emb_style)\n",
    "#print(n_input_bins, pad_value, mask_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Sample Characteristic[organism]',\n",
      "       'Sample Characteristic Ontology Term[organism]',\n",
      "       'Sample Characteristic[individual]', 'Sample Characteristic[sex]',\n",
      "       'Sample Characteristic Ontology Term[sex]',\n",
      "       'Sample Characteristic[age]',\n",
      "       'Sample Characteristic[developmental stage]',\n",
      "       'Sample Characteristic Ontology Term[developmental stage]',\n",
      "       'Sample Characteristic[organism part]',\n",
      "       'Sample Characteristic Ontology Term[organism part]',\n",
      "       'Sample Characteristic[sampling site]',\n",
      "       'Sample Characteristic Ontology Term[sampling site]',\n",
      "       'Sample Characteristic[disease]',\n",
      "       'Sample Characteristic Ontology Term[disease]',\n",
      "       'Sample Characteristic[organism status]',\n",
      "       'Sample Characteristic Ontology Term[organism status]',\n",
      "       'Factor Value[disease]', 'Factor Value Ontology Term[disease]',\n",
      "       'Factor Value[sampling site]',\n",
      "       'Factor Value Ontology Term[sampling site]',\n",
      "       'Factor Value[inferred cell type - ontology labels]',\n",
      "       'Factor Value Ontology Term[inferred cell type - ontology labels]',\n",
      "       'Factor Value[inferred cell type - authors labels]',\n",
      "       'Factor Value Ontology Term[inferred cell type - authors labels]',\n",
      "       'str_batch', 'celltype'],\n",
      "      dtype='object')\n",
      "                    index_column gene_name\n",
      "ENSG00000000971  ENSG00000000971       CFH\n",
      "ENSG00000002330  ENSG00000002330       BAD\n",
      "ENSG00000002549  ENSG00000002549      LAP3\n",
      "ENSG00000002586  ENSG00000002586      CD99\n",
      "ENSG00000002745  ENSG00000002745     WNT16\n",
      "Index(['gene_name'], dtype='object')\n",
      "['oligodendrocyte A', 'PVALB-expressing interneuron', 'oligodendrocyte precursor cell', 'mixed glial cell?', 'VIP-expressing interneuron', ..., 'SST-expressing interneuron', 'cortical layer 2-3 excitatory neuron A', 'pyramidal neuron?', 'endothelial cell', 'oligodendrocyte C']\n",
      "Length: 18\n",
      "Categories (18, object): ['PVALB-expressing interneuron', 'SST-expressing interneuron', 'SV2C-expressing interneuron', 'VIP-expressing interneuron', ..., 'oligodendrocyte C', 'oligodendrocyte precursor cell', 'phagocyte', 'pyramidal neuron?']\n",
      "[13  0 13 ...  3  8  6]\n",
      "18\n",
      "KeysView(Layers with keys: )\n",
      "Main matrix (adata.X) type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Data shape: (13468, 3000)\n",
      "Minimum value: 0.0\n",
      "Maximum value: 7.28958797454834\n",
      "\n",
      "Preprocessing info in .uns:\n",
      "dict_keys(['log1p', 'neighbors', 'umap'])\n",
      "\n",
      "New dataset size: 13468 cells x 3000 genes\n",
      "\n",
      "New distribution of cell types:\n",
      "celltype\n",
      "cortical layer 2-3 excitatory neuron B    2178\n",
      "cortical layer 5-6 excitatory neuron      1732\n",
      "cortical layer 4 excitatory neuron        1504\n",
      "astrocyte                                 1330\n",
      "oligodendrocyte A                         1216\n",
      "VIP-expressing interneuron                1158\n",
      "PVALB-expressing interneuron               878\n",
      "oligodendrocyte precursor cell             866\n",
      "mixed glial cell?                          573\n",
      "pyramidal neuron?                          528\n",
      "SV2C-expressing interneuron                344\n",
      "cortical layer 2-3 excitatory neuron A     314\n",
      "SST-expressing interneuron                 258\n",
      "mixed excitatory neuron                    185\n",
      "endothelial cell                           170\n",
      "microglial cell                            118\n",
      "phagocyte                                   82\n",
      "oligodendrocyte C                           34\n",
      "Name: count, dtype: int64\n",
      "                            Sample Characteristic[organism]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC                    Homo sapiens   \n",
      "SRR9123033-AAACCTGCAGACGCCT                    Homo sapiens   \n",
      "SRR9123033-AAAGATGGTAGCGTAG                    Homo sapiens   \n",
      "SRR9123033-AAAGATGGTGCCTGGT                    Homo sapiens   \n",
      "SRR9123033-AAAGTAGCACCAGGTC                    Homo sapiens   \n",
      "\n",
      "                             Sample Characteristic Ontology Term[organism]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC  http://purl.obolibrary.org/obo/NCBITaxon_9606   \n",
      "SRR9123033-AAACCTGCAGACGCCT  http://purl.obolibrary.org/obo/NCBITaxon_9606   \n",
      "SRR9123033-AAAGATGGTAGCGTAG  http://purl.obolibrary.org/obo/NCBITaxon_9606   \n",
      "SRR9123033-AAAGATGGTGCCTGGT  http://purl.obolibrary.org/obo/NCBITaxon_9606   \n",
      "SRR9123033-AAAGTAGCACCAGGTC  http://purl.obolibrary.org/obo/NCBITaxon_9606   \n",
      "\n",
      "                            Sample Characteristic[individual]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC                               MS1   \n",
      "SRR9123033-AAACCTGCAGACGCCT                               MS1   \n",
      "SRR9123033-AAAGATGGTAGCGTAG                               MS1   \n",
      "SRR9123033-AAAGATGGTGCCTGGT                               MS1   \n",
      "SRR9123033-AAAGTAGCACCAGGTC                               MS1   \n",
      "\n",
      "                             Sample Characteristic Ontology Term[individual]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC                                              NaN   \n",
      "SRR9123033-AAACCTGCAGACGCCT                                              NaN   \n",
      "SRR9123033-AAAGATGGTAGCGTAG                                              NaN   \n",
      "SRR9123033-AAAGATGGTGCCTGGT                                              NaN   \n",
      "SRR9123033-AAAGTAGCACCAGGTC                                              NaN   \n",
      "\n",
      "                            Sample Characteristic[sex]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC                     female   \n",
      "SRR9123033-AAACCTGCAGACGCCT                     female   \n",
      "SRR9123033-AAAGATGGTAGCGTAG                     female   \n",
      "SRR9123033-AAAGATGGTGCCTGGT                     female   \n",
      "SRR9123033-AAAGTAGCACCAGGTC                     female   \n",
      "\n",
      "                                Sample Characteristic Ontology Term[sex]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC  http://purl.obolibrary.org/obo/PATO_0000383   \n",
      "SRR9123033-AAACCTGCAGACGCCT  http://purl.obolibrary.org/obo/PATO_0000383   \n",
      "SRR9123033-AAAGATGGTAGCGTAG  http://purl.obolibrary.org/obo/PATO_0000383   \n",
      "SRR9123033-AAAGATGGTGCCTGGT  http://purl.obolibrary.org/obo/PATO_0000383   \n",
      "SRR9123033-AAAGTAGCACCAGGTC  http://purl.obolibrary.org/obo/PATO_0000383   \n",
      "\n",
      "                            Sample Characteristic[age]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC                    35 year   \n",
      "SRR9123033-AAACCTGCAGACGCCT                    35 year   \n",
      "SRR9123033-AAAGATGGTAGCGTAG                    35 year   \n",
      "SRR9123033-AAAGATGGTGCCTGGT                    35 year   \n",
      "SRR9123033-AAAGTAGCACCAGGTC                    35 year   \n",
      "\n",
      "                             Sample Characteristic Ontology Term[age]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC                                       NaN   \n",
      "SRR9123033-AAACCTGCAGACGCCT                                       NaN   \n",
      "SRR9123033-AAAGATGGTAGCGTAG                                       NaN   \n",
      "SRR9123033-AAAGATGGTGCCTGGT                                       NaN   \n",
      "SRR9123033-AAAGTAGCACCAGGTC                                       NaN   \n",
      "\n",
      "                            Sample Characteristic[developmental stage]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC                          human adult stage   \n",
      "SRR9123033-AAACCTGCAGACGCCT                          human adult stage   \n",
      "SRR9123033-AAAGATGGTAGCGTAG                          human adult stage   \n",
      "SRR9123033-AAAGATGGTGCCTGGT                          human adult stage   \n",
      "SRR9123033-AAAGTAGCACCAGGTC                          human adult stage   \n",
      "\n",
      "                            Sample Characteristic Ontology Term[developmental stage]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC      http://purl.obolibrary.org/obo/HsapDv_0000087         \n",
      "SRR9123033-AAACCTGCAGACGCCT      http://purl.obolibrary.org/obo/HsapDv_0000087         \n",
      "SRR9123033-AAAGATGGTAGCGTAG      http://purl.obolibrary.org/obo/HsapDv_0000087         \n",
      "SRR9123033-AAAGATGGTGCCTGGT      http://purl.obolibrary.org/obo/HsapDv_0000087         \n",
      "SRR9123033-AAAGTAGCACCAGGTC      http://purl.obolibrary.org/obo/HsapDv_0000087         \n",
      "\n",
      "                             ...  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC  ...   \n",
      "SRR9123033-AAACCTGCAGACGCCT  ...   \n",
      "SRR9123033-AAAGATGGTAGCGTAG  ...   \n",
      "SRR9123033-AAAGATGGTGCCTGGT  ...   \n",
      "SRR9123033-AAAGTAGCACCAGGTC  ...   \n",
      "\n",
      "                                      Factor Value Ontology Term[disease]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC  http://purl.obolibrary.org/obo/MONDO_0005301   \n",
      "SRR9123033-AAACCTGCAGACGCCT  http://purl.obolibrary.org/obo/MONDO_0005301   \n",
      "SRR9123033-AAAGATGGTAGCGTAG  http://purl.obolibrary.org/obo/MONDO_0005301   \n",
      "SRR9123033-AAAGATGGTGCCTGGT  http://purl.obolibrary.org/obo/MONDO_0005301   \n",
      "SRR9123033-AAAGTAGCACCAGGTC  http://purl.obolibrary.org/obo/MONDO_0005301   \n",
      "\n",
      "                            Factor Value[sampling site]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC             premotor cortex   \n",
      "SRR9123033-AAACCTGCAGACGCCT             premotor cortex   \n",
      "SRR9123033-AAAGATGGTAGCGTAG             premotor cortex   \n",
      "SRR9123033-AAAGATGGTGCCTGGT             premotor cortex   \n",
      "SRR9123033-AAAGTAGCACCAGGTC             premotor cortex   \n",
      "\n",
      "                                 Factor Value Ontology Term[sampling site]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC  http://purl.obolibrary.org/obo/UBERON_0016634   \n",
      "SRR9123033-AAACCTGCAGACGCCT  http://purl.obolibrary.org/obo/UBERON_0016634   \n",
      "SRR9123033-AAAGATGGTAGCGTAG  http://purl.obolibrary.org/obo/UBERON_0016634   \n",
      "SRR9123033-AAAGATGGTGCCTGGT  http://purl.obolibrary.org/obo/UBERON_0016634   \n",
      "SRR9123033-AAAGTAGCACCAGGTC  http://purl.obolibrary.org/obo/UBERON_0016634   \n",
      "\n",
      "                            Factor Value[inferred cell type - ontology labels]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC                                    oligodendrocyte   \n",
      "SRR9123033-AAACCTGCAGACGCCT                               cortical interneuron   \n",
      "SRR9123033-AAAGATGGTAGCGTAG                                    oligodendrocyte   \n",
      "SRR9123033-AAAGATGGTGCCTGGT                     oligodendrocyte precursor cell   \n",
      "SRR9123033-AAAGTAGCACCAGGTC                                    oligodendrocyte   \n",
      "\n",
      "                            Factor Value Ontology Term[inferred cell type - ontology labels]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC          http://purl.obolibrary.org/obo/CL_0000128                 \n",
      "SRR9123033-AAACCTGCAGACGCCT          http://purl.obolibrary.org/obo/CL_0008031                 \n",
      "SRR9123033-AAAGATGGTAGCGTAG          http://purl.obolibrary.org/obo/CL_0000128                 \n",
      "SRR9123033-AAAGATGGTGCCTGGT          http://purl.obolibrary.org/obo/CL_0002453                 \n",
      "SRR9123033-AAAGTAGCACCAGGTC          http://purl.obolibrary.org/obo/CL_0000128                 \n",
      "\n",
      "                            Factor Value[inferred cell type - authors labels]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC                                 oligodendrocyte A   \n",
      "SRR9123033-AAACCTGCAGACGCCT                      PVALB-expressing interneuron   \n",
      "SRR9123033-AAAGATGGTAGCGTAG                                 oligodendrocyte A   \n",
      "SRR9123033-AAAGATGGTGCCTGGT                    oligodendrocyte precursor cell   \n",
      "SRR9123033-AAAGTAGCACCAGGTC                                 oligodendrocyte A   \n",
      "\n",
      "                            Factor Value Ontology Term[inferred cell type - authors labels]  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC          http://purl.obolibrary.org/obo/CL_0000128                \n",
      "SRR9123033-AAACCTGCAGACGCCT          http://purl.obolibrary.org/obo/CL_4023018                \n",
      "SRR9123033-AAAGATGGTAGCGTAG          http://purl.obolibrary.org/obo/CL_0000128                \n",
      "SRR9123033-AAAGATGGTGCCTGGT          http://purl.obolibrary.org/obo/CL_0002453                \n",
      "SRR9123033-AAAGTAGCACCAGGTC          http://purl.obolibrary.org/obo/CL_0000128                \n",
      "\n",
      "                            str_batch                        celltype  \\\n",
      "SRR9123033-AAACCTGAGCTAGCCC         1               oligodendrocyte A   \n",
      "SRR9123033-AAACCTGCAGACGCCT         1    PVALB-expressing interneuron   \n",
      "SRR9123033-AAAGATGGTAGCGTAG         1               oligodendrocyte A   \n",
      "SRR9123033-AAAGATGGTGCCTGGT         1  oligodendrocyte precursor cell   \n",
      "SRR9123033-AAAGTAGCACCAGGTC         1               oligodendrocyte A   \n",
      "\n",
      "                             celltype_id  \n",
      "SRR9123033-AAACCTGAGCTAGCCC           13  \n",
      "SRR9123033-AAACCTGCAGACGCCT            0  \n",
      "SRR9123033-AAAGATGGTAGCGTAG           13  \n",
      "SRR9123033-AAAGATGGTGCCTGGT           15  \n",
      "SRR9123033-AAAGTAGCACCAGGTC           13  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import scanpy as sc\n",
    "\n",
    "#Input\n",
    "dataset = Path(\"scGPT_data/ms/filtered_ms_adata.h5ad\")\n",
    "adata = sc.read(dataset)\n",
    "print(adata.obs.select_dtypes(['category']).columns)\n",
    "adata.obs[\"celltype\"] = adata.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\")\n",
    "print(adata.var.head())\n",
    "print(adata.var.select_dtypes(['category']).columns) #show all the categories of the variables\n",
    "adata.var.set_index(adata.var[\"gene_name\"], inplace=True) #make sure it starts at 0 \n",
    "#some conditions about the data\n",
    "data_is_raw = False\n",
    "filter_gene_by_counts = False\n",
    "\n",
    "#make numerical id labels for the model\n",
    "celltype_id_labels = adata.obs[\"celltype\"].astype(\"category\").cat.codes.values\n",
    "celltypes = adata.obs[\"celltype\"].unique()\n",
    "print(celltypes)\n",
    "print(celltype_id_labels)\n",
    "\n",
    "num_types = len(np.unique(celltype_id_labels))\n",
    "print(num_types)\n",
    "id2type = dict(enumerate(adata.obs[\"celltype\"].astype(\"category\").cat.categories)) #mappping from id to celltype\n",
    "adata.obs[\"celltype_id\"] = celltype_id_labels\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "\n",
    "# Check the layer names (if data exists in different processing states)\n",
    "print(adata.layers.keys())\n",
    "\n",
    "# Check the main data matrix\n",
    "print(f\"Main matrix (adata.X) type: {type(adata.X)}\")\n",
    "print(f\"Data shape: {adata.shape}\")\n",
    "print(f\"Minimum value: {adata.X.min()}\")\n",
    "print(f\"Maximum value: {adata.X.max()}\")\n",
    "\n",
    "# Check if there's any preprocessing info stored in .uns\n",
    "print(\"\\nPreprocessing info in .uns:\")\n",
    "print(adata.uns.keys())\n",
    "\n",
    "print(\"\\nNew dataset size:\", adata.n_obs, \"cells x\", adata.n_vars, \"genes\")\n",
    "print(\"\\nNew distribution of cell types:\")\n",
    "print(adata.obs['celltype'].value_counts())\n",
    "\n",
    "print(adata.obs.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/__categories/Organ' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/Organ' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/__categories/Donor' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/Donor' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/__categories/Chemistry' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/Chemistry' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/__categories/Cell_category' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/Cell_category' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/__categories/Predicted_labels_CellTypist' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/Predicted_labels_CellTypist' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/__categories/Majority_voting_CellTypist' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/Majority_voting_CellTypist' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/__categories/Majority_voting_CellTypist_high' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/Majority_voting_CellTypist_high' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/__categories/Manually_curated_celltype' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/Manually_curated_celltype' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/obs/_index' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\specs\\registry.py:275: OldFormatWarning: Element '/obsm' was written without encoding metadata.\n",
      "  return self.callback(read_func, elem.name, elem, iospec=iospec)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\specs\\registry.py:275: OldFormatWarning: Element '/obsm/X_umap' was written without encoding metadata.\n",
      "  return self.callback(read_func, elem.name, elem, iospec=iospec)\n",
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\anndata\\_io\\utils.py:211: OldFormatWarning: Element '/var/_index' was written without encoding metadata.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Organ', 'Donor', 'Chemistry', 'Cell_category',\n",
      "       'Predicted_labels_CellTypist', 'Majority_voting_CellTypist',\n",
      "       'Majority_voting_CellTypist_high', 'Manually_curated_celltype'],\n",
      "      dtype='object')\n",
      "AnnData object with n_obs × n_vars = 329762 × 36601\n",
      "    obs: 'Organ', 'Donor', 'Chemistry', 'Cell_category', 'Predicted_labels_CellTypist', 'Majority_voting_CellTypist', 'Majority_voting_CellTypist_high', 'Manually_curated_celltype'\n",
      "    obsm: 'X_umap'\n",
      "Layers with keys: \n",
      "Index([], dtype='object')\n",
      "Index([], dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 36601 entries, MIR1302-2HG to AC007325.2\n",
      "Empty DataFrame\n",
      "None\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [MIR1302-2HG, FAM138A, OR4F5, AL627309.1, AL627309.3]\n",
      "Current var index:\n",
      "Index(['MIR1302-2HG', 'FAM138A', 'OR4F5', 'AL627309.1', 'AL627309.3'], dtype='object')\n",
      "               gene_name\n",
      "MIR1302-2HG  MIR1302-2HG\n",
      "FAM138A          FAM138A\n",
      "OR4F5              OR4F5\n",
      "AL627309.1    AL627309.1\n",
      "AL627309.3    AL627309.3\n",
      "\n",
      "After modification:\n",
      "Columns in var: Index([], dtype='object')\n",
      "First few genes in index: Index(['MIR1302-2HG', 'FAM138A', 'OR4F5', 'AL627309.1', 'AL627309.3'], dtype='object', name='gene_name')\n",
      "\n",
      "New dataset size: 100 cells x 36601 genes\n",
      "\n",
      "New distribution of cell types:\n",
      "celltype\n",
      "CD16+ NK cells                    11\n",
      "Cytotoxic T cells                 10\n",
      "Tcm/Naive helper T cells          10\n",
      "Memory B cells                     9\n",
      "Helper T cells                     8\n",
      "Classical monocytes                7\n",
      "Tem/Effector helper T cells        5\n",
      "Macrophages                        5\n",
      "Regulatory T cells                 5\n",
      "Follicular helper T cells          4\n",
      "Naive B cells                      4\n",
      "Plasma cells                       3\n",
      "Tem/Effector cytotoxic T cells     2\n",
      "Tcm/Naive cytotoxic T cells        2\n",
      "CD16- NK cells                     2\n",
      "NK cells                           2\n",
      "Type 17 helper T cells             1\n",
      "T cells                            1\n",
      "Memory CD4+ cytotoxic T cells      1\n",
      "Non-classical monocytes            1\n",
      "NKT cells                          1\n",
      "MAIT cells                         1\n",
      "Granulocytes                       1\n",
      "ETP                                1\n",
      "ELP                                1\n",
      "CD8a/b(entry)                      1\n",
      "gamma-delta T cells                1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "global_data = r\"C:\\Users\\annel\\OneDrive\\Documenten\\Machine Learning\\SingleCellData\\global\\global_raw.h5ad\"\n",
    "#Input\n",
    "dataset = Path(global_data)\n",
    "adata_glob = sc.read(dataset)\n",
    "print(adata_glob.obs.select_dtypes(['category']).columns)\n",
    "#adata.obs[\"celltype\"] = adata.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\")\n",
    "#print(adata.var.head())\n",
    "\n",
    "#print(adata.obs.head()) \n",
    "print(adata_glob)\n",
    "print(adata_glob.layers)\n",
    "adata_glob.obs[\"celltype\"] = adata_glob.obs[\"Predicted_labels_CellTypist\"].astype(\"category\")\n",
    "object_columns = adata_glob.var.select_dtypes(['object']).columns\n",
    "print(object_columns)\n",
    "print(adata_glob.var.columns)\n",
    "print(adata_glob.var.info())\n",
    "print(adata_glob.var.head())\n",
    "\n",
    "# First, let's check what we have\n",
    "print(\"Current var index:\")\n",
    "print(adata_glob.var.index[:5])  # Show first 5 gene names\n",
    "adata_glob.var[\"gene_name\"] = adata_glob.var.index\n",
    "print(adata_glob.var.head())\n",
    "\n",
    "# Now set this new column as the index\n",
    "adata_glob.var.set_index(\"gene_name\", inplace=True)\n",
    "\n",
    "# Verify the result\n",
    "print(\"\\nAfter modification:\")\n",
    "print(\"Columns in var:\", adata_glob.var.columns)\n",
    "print(\"First few genes in index:\", adata_glob.var.index[:5])\n",
    "\n",
    "adata_glob_small = adata_glob.copy()\n",
    "sc.pp.subsample(adata_glob_small, n_obs=100, random_state=42)\n",
    "\n",
    "print(\"\\nNew dataset size:\", adata_glob_small.n_obs, \"cells x\", adata_glob_small.n_vars, \"genes\")\n",
    "print(\"\\nNew distribution of cell types:\")\n",
    "print(adata_glob_small.obs['celltype'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 100 × 36601\n",
      "    obs: 'Organ', 'Donor', 'Chemistry', 'Cell_category', 'Predicted_labels_CellTypist', 'Majority_voting_CellTypist', 'Majority_voting_CellTypist_high', 'Manually_curated_celltype', 'celltype'\n",
      "    var: 'gene_name'\n",
      "    obsm: 'X_umap', 'bin_edges'\n",
      "    layers: 'X_normed', 'X_binned'\n",
      "AnnData object with n_obs × n_vars = 100 × 36601\n",
      "    obs: 'Organ', 'Donor', 'Chemistry', 'Cell_category', 'Predicted_labels_CellTypist', 'Majority_voting_CellTypist', 'Majority_voting_CellTypist_high', 'Manually_curated_celltype', 'celltype'\n",
      "    var: 'gene_name'\n",
      "    obsm: 'X_umap', 'bin_edges'\n",
      "    layers: 'X_normed', 'X_binned'\n",
      "               gene_name\n",
      "MIR1302-2HG  MIR1302-2HG\n",
      "FAM138A          FAM138A\n",
      "OR4F5              OR4F5\n",
      "AL627309.1    AL627309.1\n",
      "AL627309.3    AL627309.3\n",
      "DataFrame structure:\n",
      "               gene_name\n",
      "MIR1302-2HG  MIR1302-2HG\n",
      "FAM138A          FAM138A\n",
      "OR4F5              OR4F5\n",
      "AL627309.1    AL627309.1\n",
      "AL627309.3    AL627309.3\n",
      "\n",
      "Columns in var: Index(['gene_name'], dtype='object')\n",
      "Index name: None\n",
      "(100, 36601)\n",
      "Index([], dtype='object')\n",
      "['NK cells', 'CD16+ NK cells', 'Cytotoxic T cells', 'Helper T cells', 'Tcm/Naive helper T cells', ..., 'gamma-delta T cells', 'Follicular helper T cells', 'MAIT cells', 'Type 17 helper T cells', 'ETP']\n",
      "Length: 27\n",
      "Categories (27, object): ['CD16+ NK cells', 'CD16- NK cells', 'CD8a/b(entry)', 'Classical monocytes', ..., 'Tem/Effector cytotoxic T cells', 'Tem/Effector helper T cells', 'Type 17 helper T cells', 'gamma-delta T cells']\n",
      "[14  0  4  9 22 20 18 22 22  4 16 12  4 19  8  4  9  9 16  0 23 12 22 12\n",
      "  5 11 19 24 17  3 21  3 24 24 16  9 22  0 18 12 16 21  0  3 13  0  4  9\n",
      "  1  3  2 18  9 12 12  3 19 22  0 15 26 19  4  7  0  3  9 22 11 24 10  9\n",
      "  1 12 25 11 11  3 11 12  0 23  7 22  6  0 12 19 22 24 22  4  7  7  4  4\n",
      " 14  4  0  0]\n",
      "27\n",
      "KeysView(Layers with keys: X_normed, X_binned)\n"
     ]
    }
   ],
   "source": [
    "print(adata)\n",
    "print(adata_glob_small)\n",
    "adata = adata_glob_small\n",
    "print(adata.var.head())\n",
    "# First, reset the index to get the gene names as a column\n",
    "# First, let's explicitly create a new DataFrame with the gene names\n",
    "gene_names = adata.var.index.tolist()\n",
    "import pandas as pd\n",
    "\n",
    "# Create a new DataFrame with the gene names both as index and as a column\n",
    "new_var = pd.DataFrame({'gene_name': gene_names}, index=gene_names)\n",
    "\n",
    "# Replace the existing var DataFrame in the AnnData object\n",
    "adata.var = new_var\n",
    "\n",
    "# Verify our changes\n",
    "print(\"DataFrame structure:\")\n",
    "print(adata.var.head())\n",
    "print(\"\\nColumns in var:\", adata.var.columns)\n",
    "print(\"Index name:\", adata.var.index.name)\n",
    "print(adata.shape)\n",
    "\n",
    "print(adata.var.select_dtypes(['category']).columns) #show all the categories of the variables\n",
    "adata.var.set_index(adata.var[\"gene_name\"], inplace=True) #make sure it starts at 0 \n",
    "#some conditions about the data\n",
    "data_is_raw = False\n",
    "filter_gene_by_counts = False\n",
    "\n",
    "#make numerical id labels for the model\n",
    "celltype_id_labels = adata.obs[\"celltype\"].astype(\"category\").cat.codes.values\n",
    "celltypes = adata.obs[\"celltype\"].unique()\n",
    "print(celltypes)\n",
    "print(celltype_id_labels)\n",
    "\n",
    "num_types = len(np.unique(celltype_id_labels))\n",
    "print(num_types)\n",
    "id2type = dict(enumerate(adata.obs[\"celltype\"].astype(\"category\").cat.categories)) #mappping from id to celltype\n",
    "adata.obs[\"celltype_id\"] = celltype_id_labels\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "\n",
    "# Check the layer names (if data exists in different processing states)\n",
    "print(adata.layers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m         vocab\u001b[38;5;241m.\u001b[39madd_special_token(s)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#making sure all the words of the data are in the vocabulary\u001b[39;00m\n\u001b[0;32m     20\u001b[0m adata\u001b[38;5;241m.\u001b[39mvar[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_in_vocab\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gene \u001b[38;5;129;01min\u001b[39;00m vocab \n\u001b[1;32m---> 21\u001b[0m                             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m gene \u001b[38;5;129;01min\u001b[39;00m \u001b[43madata\u001b[49m\u001b[38;5;241m.\u001b[39mvar[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgene_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;66;03m#check if the gene is in the vocabulary\u001b[39;00m\n\u001b[0;32m     22\u001b[0m gene_ids_in_vocab \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(adata\u001b[38;5;241m.\u001b[39mvar[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_in_vocab\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#print how many genes are in the  vocab and how many are not\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'adata' is not defined"
     ]
    }
   ],
   "source": [
    "#configurate the model\n",
    "human_model_dir = Path(\"scGPT_data/Human\")\n",
    "model_config_file = human_model_dir / \"args.json\" #load the model configuration\n",
    "model_file = human_model_dir / \"best_model.pt\" #load the pretrained model\n",
    "vocab_file = human_model_dir / \"vocab.json\" #load the vocabulary\n",
    "\n",
    "#special tokens for padding and cls\n",
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"] #end of cell\n",
    "\n",
    "#generate a vocabubulary file\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "shutil.copy(vocab_file, \"vocab.json\") #make safety copy\n",
    "for s in special_tokens: #make sure the padding and cls is there\n",
    "    if s not in vocab:\n",
    "        vocab.add_special_token(s)\n",
    "\n",
    "#making sure all the words of the data are in the vocabulary\n",
    "adata.var[\"id_in_vocab\"] = [1 if gene in vocab \n",
    "                            else -1 for gene in adata.var[\"gene_name\"]] #check if the gene is in the vocabulary\n",
    "gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "#print how many genes are in the  vocab and how many are not\n",
    "logger.info(\n",
    "    f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "    f\"in vocabulary of size {len(vocab)}.\"\n",
    ")\n",
    "adata = adata[:, adata.var[\"id_in_vocab\"] >= 0] #filter the genes that are not in the vocabulary (only keep the ones that are in it)\n",
    "\n",
    "#Just to check quickly: \n",
    "#print(adata)\n",
    "#print (adata.var)\n",
    "\n",
    "with open(model_config_file, \"r\") as f:\n",
    "    model_configs = json.load(f) #load from the args.json file the configuration of the model\n",
    "\n",
    "#load the parameters and override the ones from the configuration given in this script, the args.json has the correct ones used for the pre-training\n",
    "embsize = model_configs[\"embsize\"] \n",
    "nheads = model_configs[\"nheads\"]\n",
    "d_hid = model_configs[\"d_hid\"]\n",
    "nlayers = model_configs[\"nlayers\"]\n",
    "n_layers_cls = model_configs[\"n_layers_cls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 8 512 12 3\n"
     ]
    }
   ],
   "source": [
    "print(embsize, nheads, d_hid, nlayers, n_layers_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Before preprocessing:\n",
      "Available layers: ['X_normed', 'X_binned']\n",
      "Main matrix range: [0.0, 7.801219463348389]\n",
      "Mean counts per cell: 2395.090087890625\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\scanpy\\preprocessing\\_normalization.py:207: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After preprocessing:\n",
      "Available layers: ['X_normed', 'X_binned']\n",
      "Normalized data range: [0.0, 60.088294982910156]\n",
      "Mean counts per cell (normalized): 10000.0\n",
      "Binned data range: [0, 180]\n"
     ]
    }
   ],
   "source": [
    "# set up the preprocessor, use the args to config the workflow. The preprocessor is a function from scgpt that preprocesses the data\n",
    "print(data_is_raw)\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning= config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "# Before preprocessing\n",
    "print(\"Before preprocessing:\")\n",
    "print(f\"Available layers: {list(adata.layers.keys())}\")\n",
    "print(f\"Main matrix range: [{adata.X.min()}, {adata.X.max()}]\")\n",
    "print(f\"Mean counts per cell: {adata.X.sum(axis=1).mean()}\")\n",
    "\n",
    "# Apply preprocessing\n",
    "preprocessor(adata)\n",
    "\n",
    "# After preprocessing\n",
    "print(\"\\nAfter preprocessing:\")\n",
    "print(f\"Available layers: {list(adata.layers.keys())}\")\n",
    "if \"X_normed\" in adata.layers:\n",
    "    print(f\"Normalized data range: [{adata.layers['X_normed'].min()}, {adata.layers['X_normed'].max()}]\")\n",
    "    print(f\"Mean counts per cell (normalized): {adata.layers['X_normed'].sum(axis=1).mean()}\")\n",
    "if \"X_binned\" in adata.layers:\n",
    "    print(f\"Binned data range: [{adata.layers['X_binned'].min()}, {adata.layers['X_binned'].max()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#in case there is no config of the model, we will get them from the genes we found in the anndata \n",
    "print (config.load_model is None)\n",
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "\n",
    "#define the genes from the anndata\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "#set the default index to padding\n",
    "vocab.set_default_index(vocab[\"<pad>\"]) #following the \"grammatical rules\" that it has to start with padding to ensure all the same length\n",
    "gene_ids = np.array(vocab(genes), dtype=int) #create the gene ids from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset): \n",
    "    def __init__(self, data, labels=None): #data is a dictionary of different tensors, all share same first dimension\n",
    "        self.data = data # dictionary with keys: gene_ids, values\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0] #number of samples (first dimension of data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()} #return a tuple of data and labels of the sample\n",
    "        #this is a dictionary\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration values:\n",
      "embsize: 512\n",
      "nhead: 8\n",
      "d_hid: 512\n",
      "nlayers: 12\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Add these debug prints before model creation\n",
    "print(\"Configuration values:\")\n",
    "print(f\"embsize: {embsize}\")\n",
    "print(f\"nhead: {nheads}\")\n",
    "print(f\"d_hid: {d_hid}\")\n",
    "print(f\"nlayers: {nlayers}\")\n",
    "\n",
    "# Then create model\n",
    "print (all_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the pre-trained scGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\scgpt\\model\\model.py:77: UserWarning: flash-attn is not installed, using pytorch transformer instead. Set use_fast_transformer=False to avoid this warning. Installing flash-attn is highly recommended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): GeneEncoder(\n",
       "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
       "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_encoder): ContinuousValueEncoder(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ExprDecoder(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (cls_decoder): ClsDecoder(\n",
       "    (_decoder): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_layer): Linear(in_features=512, out_features=27, bias=True)\n",
       "  )\n",
       "  (sim): Similarity(\n",
       "    (cos): CosineSimilarity()\n",
       "  )\n",
       "  (creterion_cce): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)\n",
    "\n",
    "\n",
    "ntokens = len(vocab) #use to define the input length of the model\n",
    "\n",
    "# Now initialize the model with all parameters\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nheads,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=num_types,  # This will use your 18 cell types\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=MVC,\n",
    "    do_dab=DAB,\n",
    "    use_batch_labels=INPUT_BATCH_LABELS,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=config.DSBN,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    mvc_decoder_style=mvc_decoder_style,\n",
    "    ecs_threshold=ecs_threshold,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "\n",
    "if config.load_model is not None:\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = torch.load(model_file, map_location=device)\n",
    "    pretrained_dict = {\n",
    "        k: v\n",
    "        for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and v.shape == model_dict[k].shape\n",
    "    }\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()  # Add this line for inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Configuration ===\n",
      "Embedding size (inferred): 512\n",
      "Number of heads (inferred): 1\n",
      "Number of transformer layers (inferred): 12\n"
     ]
    }
   ],
   "source": [
    "def print_model_config(model):\n",
    "    \"\"\"Print the configuration of a loaded model\"\"\"\n",
    "    print(\"\\n=== Model Configuration ===\")\n",
    "    \n",
    "    # Get model state dict\n",
    "    state_dict = model.state_dict()\n",
    "    \n",
    "    # Infer embedding size from encoder embedding layer\n",
    "    emb_weight = state_dict.get('encoder.embedding.weight', None)\n",
    "    if emb_weight is not None:\n",
    "        print(f\"Embedding size (inferred): {emb_weight.shape[1]}\")\n",
    "    \n",
    "    # Correctly infer number of heads from input projection weight\n",
    "    for key, value in state_dict.items():\n",
    "        if 'self_attn.in_proj_weight' in key:\n",
    "            # Shape is [3 * d_model * nhead, d_model]\n",
    "            in_proj_shape = value.shape\n",
    "            n_heads = in_proj_shape[0] // (3 * in_proj_shape[1])\n",
    "            print(f\"Number of heads (inferred): {n_heads}\")\n",
    "            break\n",
    "    \n",
    "    # Correctly count transformer layers\n",
    "    unique_layers = set()\n",
    "    for key in state_dict.keys():\n",
    "        if 'transformer_encoder.layers.' in key:\n",
    "            layer_num = int(key.split('.')[2])\n",
    "            unique_layers.add(layer_num)\n",
    "    print(f\"Number of transformer layers (inferred): {len(unique_layers)}\")\n",
    "\n",
    "# Add this after model loading\n",
    "print_model_config(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Attention Parameter Analysis ===\n",
      "\n",
      "Analyzing transformer_encoder.layers.0.self_attn.in_proj_weight:\n",
      "Shape: torch.Size([1536, 512])\n",
      "d_model (embedding dimension): 512\n",
      "Total projection dimension: 1536\n",
      "Implies number of heads: 8.0\n",
      "\n",
      "Found MultiheadAttention in transformer_encoder.layers.0.self_attn\n",
      "Actual number of heads: 8\n",
      "Embedding dimension: 512\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def debug_attention_params(model):\n",
    "    \"\"\"Debug attention parameters in detail\"\"\"\n",
    "    print(\"\\n=== Attention Parameter Analysis ===\")\n",
    "    state_dict = model.state_dict()\n",
    "    \n",
    "    for key, value in state_dict.items():\n",
    "        if 'self_attn.in_proj_weight' in key:\n",
    "            print(f\"\\nAnalyzing {key}:\")\n",
    "            print(f\"Shape: {value.shape}\")\n",
    "            # For MultiheadAttention, in_proj_weight shape should be [3 * d_model, d_model]\n",
    "            # where d_model = embsize = nhead * head_dim\n",
    "            d_model = value.shape[1]  # 512\n",
    "            total_proj = value.shape[0]  # 1536\n",
    "            print(f\"d_model (embedding dimension): {d_model}\")\n",
    "            print(f\"Total projection dimension: {total_proj}\")\n",
    "            print(f\"Implies number of heads: {(total_proj/3)/d_model * d_model/64}\")  # Standard head_dim is usually 64\n",
    "            break\n",
    "\n",
    "    # Also check the actual MultiheadAttention parameters\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.MultiheadAttention):\n",
    "            print(f\"\\nFound MultiheadAttention in {name}\")\n",
    "            print(f\"Actual number of heads: {module.num_heads}\")\n",
    "            print(f\"Embedding dimension: {module.embed_dim}\")\n",
    "            break\n",
    "\n",
    "print (debug_attention_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "#This needs to be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix  \n",
    "\n",
    "# Set up the evaluation function\n",
    "\n",
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "\n",
    "celltypes_labels = adata.obs[\"celltype_id\"].tolist()\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "tokenized_inference = tokenize_and_pad_batch(\n",
    "    all_counts,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token= pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "\n",
    "#print (tokenized_inference)\n",
    "\n",
    "input_values_inference = random_mask_value(\n",
    "    tokenized_inference[\"values\"],\n",
    "    mask_ratio=mask_ratio,\n",
    "    mask_value=mask_value,\n",
    "    pad_value=pad_value,\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "inference_data_pt = {\n",
    "    \"gene_ids\": tokenized_inference[\"genes\"],\n",
    "    \"values\": input_values_inference,\n",
    "    \"target_values\": tokenized_inference[\"values\"],\n",
    "    \"celltypes_labels\": torch.from_numpy(celltypes_labels).long(),  # Changed from celltypes_labels to celltype_labels\n",
    "}\n",
    "\n",
    "# Add diagnostic code HERE\n",
    "print(\"Checking label distributions:\")\n",
    "print(f\"Unique celltype labels: {np.unique(celltypes_labels)}\")\n",
    "print(f\"Min label: {np.min(celltypes_labels)}\")\n",
    "print(f\"Max label: {np.max(celltypes_labels)}\")\n",
    "\n",
    "# Let's also check the model's output dimension\n",
    "print(\"\\nChecking model output dimension:\")\n",
    "for batch_data in DataLoader(SeqDataset(inference_data_pt), batch_size=1):\n",
    "    with torch.no_grad():\n",
    "        output_dict = model(\n",
    "            batch_data[\"gene_ids\"].to(device),\n",
    "            batch_data[\"values\"].to(device),\n",
    "            src_key_padding_mask=batch_data[\"gene_ids\"].eq(vocab[pad_token]).to(device),\n",
    "            CLS=True,  # Add this parameter\n",
    "            CCE=False,\n",
    "            MVC=False,\n",
    "            ECS=False,\n",
    "            do_sample=False\n",
    "        )\n",
    "        print(f\"Model output shape: {output_dict['cls_output'].shape}\")\n",
    "        print(f\"Number of classes in model output: {output_dict['cls_output'].shape[1]}\")\n",
    "        break\n",
    "\n",
    "# Print the celltypes mapping\n",
    "print(\"\\nCell type mapping:\")\n",
    "for idx, cell_type in id2type.items():\n",
    "    print(f\"ID {idx}: {cell_type}\")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset = SeqDataset(inference_data_pt),\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader, early_stop: bool = True,  return_raw: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    predictions = []\n",
    "    \n",
    "    # Add a batch counter\n",
    "    total_batches = len(loader)\n",
    "    print(f\"Starting evaluation on {total_batches} batches...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(loader):\n",
    "            # Print progress every few batches\n",
    "            if early_stop and batch_idx >= 10:\n",
    "                break\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Processing batch {batch_idx}/{total_batches}\")\n",
    "            \n",
    "            # Verify input shapes\n",
    "            print(f\"Batch {batch_idx} shapes:\")\n",
    "            print(f\"  gene_ids: {batch_data['gene_ids'].shape}\")\n",
    "            print(f\"  values: {batch_data['values'].shape}\")\n",
    "            print(f\"  target_values: {batch_data['target_values'].shape}\")\n",
    "            print(f\"  celltypes_labels: {batch_data['celltypes_labels'].shape}\")\n",
    "            \n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device) \n",
    "            celltypes_labels = batch_data[\"celltypes_labels\"].to(device)\n",
    "\n",
    "            # Check for NaN values\n",
    "            if torch.isnan(input_values).any():\n",
    "                print(f\"Warning: NaN values detected in input_values in batch {batch_idx}\")\n",
    "            \n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            \n",
    "            try:\n",
    "                with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                    output_dict = model(\n",
    "                        input_gene_ids,\n",
    "                        input_values,\n",
    "                        src_key_padding_mask=src_key_padding_mask,\n",
    "                        batch_labels = None,\n",
    "                        CLS=CLS,\n",
    "                        CCE=False,\n",
    "                        MVC=False,\n",
    "                        ECS=False,\n",
    "                        do_sample=do_sample_in_train,\n",
    "                    )\n",
    "                    \n",
    "                    # Verify output shapes\n",
    "                    print(f\"  Output shapes:\")\n",
    "                    print(f\"    cls_output: {output_dict['cls_output'].shape}\")\n",
    "                    \n",
    "                    output_values = output_dict[\"cls_output\"]\n",
    "                    loss = criterion_cls(output_values, celltypes_labels)\n",
    "\n",
    "                    # Print periodic updates about predictions\n",
    "                    if batch_idx % 10 == 0:\n",
    "                        pred_classes = output_values.argmax(1)\n",
    "                        print(f\"  Sample predictions: {pred_classes[:5]}\")\n",
    "                        print(f\"  Sample true labels: {celltypes_labels[:5]}\")\n",
    "                        print(f\"  Current batch loss: {loss.item():.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                raise e\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            accuracy = (output_values.argmax(1) == celltypes_labels).sum().item()\n",
    "            total_error += (1 - accuracy / len(input_gene_ids)) * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "            preds = output_values.argmax(1).cpu().numpy()\n",
    "            predictions.append(preds)\n",
    "\n",
    "            # Print running accuracy every few batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                current_acc = accuracy / len(input_gene_ids)\n",
    "                print(f\"  Current batch accuracy: {current_acc:.4f}\")\n",
    "\n",
    "    print(\"\\nEvaluation completed!\")\n",
    "    print(f\"Total samples processed: {total_num}\")\n",
    "    \n",
    "    if return_raw:\n",
    "        final_predictions = np.concatenate(predictions, axis=0)\n",
    "        print(f\"Final predictions shape: {final_predictions.shape}\")\n",
    "        return final_predictions\n",
    "\n",
    "    final_loss = total_loss / total_num\n",
    "    final_error = total_error / total_num\n",
    "    print(f\"Final average loss: {final_loss:.4f}\")\n",
    "    print(f\"Final average error: {final_error:.4f}\")\n",
    "    return final_loss, final_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model: nn.Module, loader: DataLoader, num_batches: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Quick evaluation of the model on a few batches to check predictions.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        loader: DataLoader containing evaluation data\n",
    "        num_batches: Number of batches to evaluate (default 10)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_accuracy = 0\n",
    "    samples_seen = 0\n",
    "    \n",
    "    print(f\"Evaluating {num_batches} batches...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(loader):\n",
    "            if batch_idx >= num_batches:\n",
    "                break\n",
    "                \n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            celltypes_labels = batch_data[\"celltypes_labels\"].to(device)\n",
    "            \n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            \n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=None,\n",
    "                CLS=CLS,\n",
    "                CCE=False,\n",
    "                MVC=False,\n",
    "                ECS=False,\n",
    "                do_sample=do_sample_in_train,\n",
    "            )\n",
    "            \n",
    "            output_values = output_dict[\"cls_output\"]\n",
    "            predictions = output_values.argmax(1)\n",
    "            \n",
    "            # Calculate accuracy for this batch\n",
    "            correct = (predictions == celltypes_labels).sum().item()\n",
    "            batch_accuracy = correct / len(celltypes_labels)\n",
    "            total_accuracy += correct\n",
    "            samples_seen += len(celltypes_labels)\n",
    "            \n",
    "            # Print predictions vs actual for this batch\n",
    "            print(f\"\\nBatch {batch_idx + 1}:\")\n",
    "            print(\"Predictions:\", predictions[:5].cpu().numpy())\n",
    "            print(\"True labels:\", celltypes_labels[:5].cpu().numpy())\n",
    "            print(f\"Batch accuracy: {batch_accuracy:.4f}\")\n",
    "    \n",
    "    # Print final accuracy\n",
    "    final_accuracy = total_accuracy / samples_seen\n",
    "    print(f\"\\nFinal accuracy over {samples_seen} samples: {final_accuracy:.4f}\")\n",
    "    \n",
    "    return final_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on 421 batches...\n",
      "Processing batch 0/421\n",
      "Batch 0 shapes:\n",
      "  gene_ids: torch.Size([32, 1401])\n",
      "  values: torch.Size([32, 1401])\n",
      "  target_values: torch.Size([32, 1401])\n",
      "  celltypes_labels: torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Output shapes:\n",
      "    cls_output: torch.Size([32, 18])\n",
      "  Sample predictions: tensor([0, 0, 0, 7, 0])\n",
      "  Sample true labels: tensor([13,  0, 13, 15, 13])\n",
      "  Current batch loss: 3.3542\n",
      "  Current batch accuracy: 0.0625\n",
      "Batch 1 shapes:\n",
      "  gene_ids: torch.Size([32, 1401])\n",
      "  values: torch.Size([32, 1401])\n",
      "  target_values: torch.Size([32, 1401])\n",
      "  celltypes_labels: torch.Size([32])\n",
      "  Output shapes:\n",
      "    cls_output: torch.Size([32, 18])\n",
      "Batch 2 shapes:\n",
      "  gene_ids: torch.Size([32, 1401])\n",
      "  values: torch.Size([32, 1401])\n",
      "  target_values: torch.Size([32, 1401])\n",
      "  celltypes_labels: torch.Size([32])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[205], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test on 10 batches (default)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Or test on fewer/more batches\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#accuracy = evaluate(model, test_loader, num_batches=5)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[199], line 44\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, loader, early_stop, return_raw)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mamp):\n\u001b[1;32m---> 44\u001b[0m         output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_gene_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCLS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCCE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43mMVC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[43mECS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample_in_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;66;03m# Verify output shapes\u001b[39;00m\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Output shapes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\scgpt\\model\\model.py:345\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, src, values, src_key_padding_mask, batch_labels, CLS, CCE, MVC, ECS, do_sample)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    316\u001b[0m     src: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m     do_sample: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[0;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m        src (:obj:`Tensor`): token ids, shape [batch_size, seq_len]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03m        dict of output Tensors.\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 345\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batch_labels:\n\u001b[0;32m    349\u001b[0m         batch_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encoder(batch_labels)  \u001b[38;5;66;03m# (batch, embsize)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\scgpt\\model\\model.py:194\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[1;34m(self, src, values, src_key_padding_mask, batch_labels)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(total_embs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 194\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_embs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:387\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    384\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 387\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    390\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:678\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m why_not_sparsity_fast_path:\n\u001b[0;32m    677\u001b[0m         merged_mask, mask_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mmerge_masks(src_mask, src_key_padding_mask, src)\n\u001b[1;32m--> 678\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformer_encoder_layer_fwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m            \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_relu_or_gelu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmerged_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m x \u001b[38;5;241m=\u001b[39m src\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test on 10 batches (default)\n",
    "accuracy = evaluate(model, test_loader)\n",
    "\n",
    "# Or test on fewer/more batches\n",
    "#accuracy = evaluate(model, test_loader, num_batches=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on 4 batches...\n",
      "Processing batch 0/4\n",
      "Batch 0 shapes:\n",
      "  gene_ids: torch.Size([32, 3001])\n",
      "  values: torch.Size([32, 3001])\n",
      "  target_values: torch.Size([32, 3001])\n",
      "  celltypes_labels: torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 0: [enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9222145024 bytes.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9222145024 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[317], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[306], line 72\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, loader, early_stop, return_raw)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     74\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_gene_ids)\n\u001b[0;32m     75\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m (output_values\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m celltypes_labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[1;32mIn[306], line 44\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, loader, early_stop, return_raw)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mamp):\n\u001b[1;32m---> 44\u001b[0m         output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_gene_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCLS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCCE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43mMVC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[43mECS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample_in_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;66;03m# Verify output shapes\u001b[39;00m\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Output shapes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\scgpt\\model\\model.py:345\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, src, values, src_key_padding_mask, batch_labels, CLS, CCE, MVC, ECS, do_sample)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    316\u001b[0m     src: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m     do_sample: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[0;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m        src (:obj:`Tensor`): token ids, shape [batch_size, seq_len]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03m        dict of output Tensors.\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 345\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batch_labels:\n\u001b[0;32m    349\u001b[0m         batch_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encoder(batch_labels)  \u001b[38;5;66;03m# (batch, embsize)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\scgpt\\model\\model.py:194\u001b[0m, in \u001b[0;36mTransformerModel._encode\u001b[1;34m(self, src, values, src_key_padding_mask, batch_labels)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(total_embs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 194\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_embs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:387\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    384\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 387\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    390\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:678\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m why_not_sparsity_fast_path:\n\u001b[0;32m    677\u001b[0m         merged_mask, mask_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mmerge_masks(src_mask, src_key_padding_mask, src)\n\u001b[1;32m--> 678\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformer_encoder_layer_fwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m            \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_relu_or_gelu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmerged_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m x \u001b[38;5;241m=\u001b[39m src\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9222145024 bytes."
     ]
    }
   ],
   "source": [
    "predictions = evaluate(model, test_loader, return_raw=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [13468, 320]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#accuracy = accuracy_score(celltypes_labels, predictions)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcelltypes_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(celltypes_labels, predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m macro_f1 \u001b[38;5;241m=\u001b[39m f1_score(celltypes_labels, predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2247\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   2079\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   2080\u001b[0m     {\n\u001b[0;32m   2081\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2106\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2107\u001b[0m ):\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   2109\u001b[0m \n\u001b[0;32m   2110\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   2246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2247\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2252\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1830\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1662\u001b[0m \n\u001b[0;32m   1663\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1829\u001b[0m _check_zero_division(zero_division)\n\u001b[1;32m-> 1830\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1832\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1833\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1596\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m   1595\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[1;32m-> 1596\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m _tolist(unique_labels(y_true, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:98\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m---> 98\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [13468, 320]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#accuracy = accuracy_score(celltypes_labels, predictions)\n",
    "precision = precision_score(celltypes_labels, predictions, average=\"macro\")\n",
    "recall = recall_score(celltypes_labels, predictions, average=\"macro\")\n",
    "macro_f1 = f1_score(celltypes_labels, predictions, average=\"macro\")\n",
    "\n",
    "#print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")   \n",
    "print(criterion_cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters: 135/169\n",
      "Model config: {'data_source': '/scratch/ssd004/datasets/cellxgene/scb_strict/human', 'save_dir': '/scratch/ssd004/datasets/cellxgene/save/cellxgene_census_human-May23-08-36-2023', 'load_model': None, 'n_hvg': None, 'valid_size_or_ratio': 0.003, 'dist_backend': 'nccl', 'grad_accu_steps': 1, 'pad_token': '<pad>', 'input_style': 'binned', 'input_emb_style': 'continuous', 'n_bins': 51, 'max_seq_len': 1200, 'training_tasks': 'both', 'dist_url': 'tcp://gpu188.cluster.local:53833', 'mask_ratio': [0.25, 0.5, 0.75], 'trunc_by_sample': True, 'vocab_path': '/scratch/ssd004/datasets/cellxgene/scFormer/scformer/tokenizer/default_census_vocab.json', 'rank': 0, 'batch_size': 32, 'eval_batch_size': 64, 'epochs': 6, 'lr': 0.0001, 'scheduler_interval': 100, 'scheduler_factor': 0.99, 'warmup_ratio_or_step': 10000.0, 'no_cls': True, 'no_cce': True, 'fp16': True, 'fast_transformer': True, 'nlayers': 12, 'nheads': 8, 'embsize': 512, 'd_hid': 512, 'dropout': 0.2, 'n_layers_cls': 3, 'log_interval': 9000, 'save_interval': 27000, 'mask_value': -1, 'pad_value': -2, 'USE_CLS': False, 'USE_CCE': False, 'MVC': True, 'USE_GENERATIVE_TRAINING': True, 'world_size': 16, 'distributed': True, 'local_rank': 0, 'gpu': 0}\n"
     ]
    }
   ],
   "source": [
    "# Add this after loading the model\n",
    "loaded_params = len(pretrained_dict.keys())\n",
    "model_params = len(model.state_dict().keys())\n",
    "print(f\"Loaded parameters: {loaded_params}/{model_params}\")\n",
    "print(f\"Model config:\", model_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (13468, 2808)\n",
      "Number of unique cell types: 18\n",
      "Cell type distribution: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17]), array([ 878,  258,  344, 1158, 1330,  314, 2178, 1504, 1732,  170,  118,\n",
      "        185,  573, 1216,   34,  866,   82,  528], dtype=int64))\n",
      "Input value range: tensor(-2.) tensor(7.2896)\n",
      "Gene IDs range: tensor(3) tensor(60695)\n",
      "Label range: 0 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape:\", adata.shape)\n",
    "print(\"Number of unique cell types:\", len(np.unique(celltypes_labels)))\n",
    "print(\"Cell type distribution:\", np.unique(celltypes_labels, return_counts=True))\n",
    "\n",
    "# Add after preprocessing\n",
    "print(\"Input value range:\", input_values_inference.min(), input_values_inference.max())\n",
    "print(\"Gene IDs range:\", tokenized_inference[\"genes\"].min(), tokenized_inference[\"genes\"].max())\n",
    "\n",
    "# Check if labels match the model's expected range\n",
    "print(\"Label range:\", celltypes_labels.min(), celltypes_labels.max())\n",
    "#print(\"Number of classes in model:\", model.n_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [13468, 320]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m celltypes:\n\u001b[0;32m      9\u001b[0m         celltypes\u001b[38;5;241m.\u001b[39mremove(i)\n\u001b[1;32m---> 10\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcelltypes_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m cm \u001b[38;5;241m=\u001b[39m cm\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m cm\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[0;32m     12\u001b[0m cm \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(cm, index\u001b[38;5;241m=\u001b[39mcelltypes[:cm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]], columns\u001b[38;5;241m=\u001b[39mcelltypes[:cm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:340\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03mBy definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m(np.int64(0), np.int64(2), np.int64(1), np.int64(1))\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[1;32m--> 340\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:98\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m---> 98\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\annel\\anaconda3\\envs\\scgpt_py39\\lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [13468, 320]"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "celltypes = list(celltypes)\n",
    "for i in set([id2type[p] for p in predictions]):\n",
    "    if i not in celltypes:\n",
    "        celltypes.remove(i)\n",
    "cm = confusion_matrix(celltypes_labels, predictions)\n",
    "cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm = pd.DataFrame(cm, index=celltypes[:cm.shape[0]], columns=celltypes[:cm.shape[1]])\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\".1f\", cmap=\"Blues\")\n",
    "#plt.savefig(save_dir / \"confusion_matrix.png\", dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: pyramidal neuron? | True: oligodendrocyte A\n",
      "Predicted: pyramidal neuron? | True: PVALB-expressing interneuron\n",
      "Predicted: pyramidal neuron? | True: oligodendrocyte A\n",
      "Predicted: pyramidal neuron? | True: oligodendrocyte precursor cell\n",
      "Predicted: pyramidal neuron? | True: oligodendrocyte A\n",
      "Predicted: pyramidal neuron? | True: mixed glial cell?\n",
      "Predicted: pyramidal neuron? | True: mixed glial cell?\n",
      "Predicted: pyramidal neuron? | True: VIP-expressing interneuron\n",
      "Predicted: pyramidal neuron? | True: astrocyte\n",
      "Predicted: pyramidal neuron? | True: oligodendrocyte precursor cell\n"
     ]
    }
   ],
   "source": [
    "# Convert numeric predictions to cell type names\n",
    "predicted_celltypes = [id2type[pred] for pred in predictions]\n",
    "true_celltypes = [id2type[label] for label in celltypes_labels]\n",
    "\n",
    "# Show first 10 predictions vs true labels\n",
    "for i in range(10):\n",
    "    print(f\"Predicted: {predicted_celltypes[i]} | True: {true_celltypes[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction probabilities distribution:\n",
      "tensor([0.0219, 0.0469, 0.0229, 0.0520, 0.0406, 0.0360, 0.0382, 0.0384, 0.0222,\n",
      "        0.0476, 0.0634, 0.0143, 0.0417, 0.0402, 0.1229, 0.1093, 0.0734, 0.1681])\n",
      "\n",
      "Cell type mapping:\n",
      "0: PVALB-expressing interneuron\n",
      "1: SST-expressing interneuron\n",
      "2: SV2C-expressing interneuron\n",
      "3: VIP-expressing interneuron\n",
      "4: astrocyte\n",
      "5: cortical layer 2-3 excitatory neuron A\n",
      "6: cortical layer 2-3 excitatory neuron B\n",
      "7: cortical layer 4 excitatory neuron\n",
      "8: cortical layer 5-6 excitatory neuron\n",
      "9: endothelial cell\n",
      "10: microglial cell\n",
      "11: mixed excitatory neuron\n",
      "12: mixed glial cell?\n",
      "13: oligodendrocyte A\n",
      "14: oligodendrocyte C\n",
      "15: oligodendrocyte precursor cell\n",
      "16: phagocyte\n",
      "17: pyramidal neuron?\n"
     ]
    }
   ],
   "source": [
    "# Look at raw prediction probabilities for one batch\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(test_loader))\n",
    "    output_dict = model(\n",
    "        batch[\"gene_ids\"].to(device),\n",
    "        batch[\"values\"].to(device),\n",
    "        src_key_padding_mask=batch[\"gene_ids\"].eq(vocab[pad_token]).to(device),\n",
    "        CLS=True,\n",
    "        CCE=False,\n",
    "        MVC=False,\n",
    "        ECS=False,\n",
    "        do_sample=False\n",
    "    )\n",
    "    probs = F.softmax(output_dict[\"cls_output\"], dim=1)\n",
    "    \n",
    "print(\"Prediction probabilities distribution:\")\n",
    "print(probs[0])  # Look at first sample's probabilities across classes\n",
    "\n",
    "# Check the mapping between indices and cell types\n",
    "print(\"\\nCell type mapping:\")\n",
    "for idx, cell_type in id2type.items():\n",
    "    print(f\"{idx}: {cell_type}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
